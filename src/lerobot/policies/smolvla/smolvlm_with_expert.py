# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import copy
import os
import builtins
from typing import List, Optional, Union

import torch
from torch import nn
from transformers import (
    AutoConfig,
    AutoModel,
    AutoModelForImageTextToText,
    AutoProcessor,
)
from .smolvlm_utils import apply_rope, get_intermediate_size
from .attention import AttentionBackend
from .cuda_safe_layers import CudaSafeLinear
from .ops import custom_mlp_forward, custom_linear_forward
from .blocks import CrossAttnBlocks

# Try to import SmolVLMForConditionalGeneration, fallback to None if not available
try:
    from transformers import SmolVLMForConditionalGeneration
except ImportError:
    SmolVLMForConditionalGeneration = None

# å°è¯•å¯¼å…¥ SmolVLM è¡¥ä¸
try:
    # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    patch_path = os.path.join(current_dir, "smolvlm_compatibility_patch.py")
    
    if os.path.exists(patch_path):
        import importlib.util
        spec = importlib.util.spec_from_file_location("smolvlm_patch", patch_path)
        smolvlm_patch = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(smolvlm_patch)
        SmolVLMForConditionalGeneration = smolvlm_patch.SmolVLMForConditionalGeneration
        print("âœ… SmolVLM è¡¥ä¸å·²åŠ è½½")
    else:
        print("âš ï¸ æœªæ‰¾åˆ° SmolVLM è¡¥ä¸æ–‡ä»¶")
except Exception as e:
    print(f"âš ï¸ SmolVLM è¡¥ä¸åŠ è½½å¤±è´¥: {e}")


def disable_cublas_globally():
    """é»˜è®¤ä¸ç¦ç”¨ cuBLASï¼›ä»…åœ¨è®¾ç½® SMOLVLA_DISABLE_CUBLAS=1 æ—¶ç¦ç”¨ã€‚"""
    if os.getenv('SMOLVLA_DISABLE_CUBLAS', '0') != '1':
        return
    try:
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False
        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False
        torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False
        os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        torch.backends.cudnn.enabled = False
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
        print("ğŸ”§ å·²æŒ‰ç¯å¢ƒå˜é‡ç¦ç”¨ cuBLAS ç›¸å…³ä¼˜åŒ–")
    except Exception as e:
        print(f"âš ï¸ å…¨å±€ç¦ç”¨ cuBLAS å¤±è´¥: {e}")


def apply_rope(*args, **kwargs):
    from .smolvlm_utils import apply_rope as _apply
    return _apply(*args, **kwargs)


def get_intermediate_size(*args, **kwargs):
    from .smolvlm_utils import get_intermediate_size as _gis
    return _gis(*args, **kwargs)


class SmolVLMWithExpertModel(nn.Module):
    def __init__(
        self,
        model_id: str = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct",
        load_vlm_weights: bool = True,
        train_expert_only: bool = True,
        freeze_vision_encoder: bool = False,
        attention_mode: str = "self_attn",
        num_expert_layers: int = -1,
        num_vlm_layers: int = -1,
        self_attn_every_n_layers: int = -1,
        expert_width_multiplier: float = 0.5,
        max_seq_length: int = 128,  # ğŸš€ æ–°å¢ï¼šåºåˆ—é•¿åº¦ä¼˜åŒ–é…ç½®
    ):
        super().__init__()
        # Lightweight debug print toggle (default off). Set SMOLVLA_VERBOSE=1 to enable.
        global _SMOLVLA_VERBOSE
        _SMOLVLA_VERBOSE = int(os.getenv("SMOLVLA_VERBOSE", "0"))
        def dbg_print(*args, **kwargs):
            if _SMOLVLA_VERBOSE:
                builtins.print(*args, **kwargs)
        self._dbg_print = dbg_print
        
        # ğŸš€ åºåˆ—é•¿åº¦ä¼˜åŒ–é…ç½®
        self.max_seq_length = max_seq_length
        # silent: do not print optimization banners by default
        # æ‰“å°æ§åˆ¶ï¼šè¶…è¿‡é™åˆ¶ä¸æˆªæ–­åªæç¤ºä¸€æ¬¡
        self._seq_len_warn_printed = False
        self._seq_len_truncate_printed = False
        
        # ä¾èµ–ç³»ç»Ÿ CUDA/cuBLASï¼Œå–æ¶ˆæ­¤å‰çš„å…¨å±€ç¦ç”¨é€»è¾‘
        
        if load_vlm_weights:
            print(f"Loading  {model_id} weights ...")
            try:
                # å°è¯•ç›´æ¥åŠ è½½ SmolVLM æ¨¡å‹
                if "smolvlm" in model_id.lower() or os.path.exists(os.path.join(model_id, "config.json")):
                    # æ£€æŸ¥é…ç½®æ–‡ä»¶
                    config_path = os.path.join(model_id, "config.json")
                    if os.path.exists(config_path):
                        import json
                        with open(config_path, 'r') as f:
                            config_data = json.load(f)
                        
                        if config_data.get("model_type") == "smolvlm":
                            print("âœ… æ£€æµ‹åˆ° SmolVLM æ¨¡å‹ï¼Œä½¿ç”¨å…¼å®¹æ€§åŠ è½½...")
                            # åˆ›å»ºå…¼å®¹çš„é…ç½®
                            from transformers import PretrainedConfig
                            config = PretrainedConfig()
                            config.model_type = "smolvlm"
                            config.hidden_size = 960
                            config.num_hidden_layers = 16
                            config.num_attention_heads = 15
                            config.intermediate_size = 3840
                            config.hidden_act = "gelu"
                            config.max_position_embeddings = 2048
                            config.initializer_range = 0.02
                            config.layer_norm_epsilon = 1e-5
                            config.use_cache = True
                            config.bos_token_id = 1
                            config.eos_token_id = 2
                            config.pad_token_id = 0
                            config.vocab_size = 49280
                            config.num_key_value_heads = 15
                            config.head_dim = 64
                            config.attention_bias = False
                            config.text_config = config
                            
                            # åˆ›å»ºçœŸå®çš„ SmolVLM æ¨¡å‹ç»“æ„
                            class RealSmolVLM(nn.Module):
                                def __init__(self, config):
                                    super().__init__()
                                    self.config = config
                                    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)
                                    
                                    # æ–‡æœ¬æ¨¡å‹
                                    class TextModel(nn.Module):
                                        def __init__(self, config):
                                            super().__init__()
                                            self.config = config
                                            # æ·»åŠ ç¼ºå¤±çš„ embed_tokens å±æ€§
                                            self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
                                            # åˆ›å»ºæ­£ç¡®çš„ Transformer å±‚ç»“æ„ï¼Œè€Œä¸æ˜¯ç®€å•çš„ Linear å±‚
                                            self.layers = nn.ModuleList()
                                            for _ in range(config.num_hidden_layers):
                                                layer = nn.TransformerEncoderLayer(
                                                    d_model=config.hidden_size,
                                                    nhead=config.num_attention_heads,
                                                    dim_feedforward=config.intermediate_size,
                                                    dropout=0.0,
                                                    activation=config.hidden_act,
                                                    batch_first=True
                                                )
                                                # æ·»åŠ  GPT é£æ ¼çš„å±æ€§ä»¥ä¿æŒå…¼å®¹æ€§
                                                layer.input_layernorm = layer.norm1
                                                layer.post_attention_layernorm = layer.norm2
                                                
                                                # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿æ¯ä¸€å±‚éƒ½æœ‰æ­£ç¡®çš„æŠ•å½±å±‚å±æ€§
                                                # åˆ›å»ºåˆ†ç¦»çš„æŠ•å½±å±‚ä»¥ä¿æŒå…¼å®¹æ€§
                                                embed_dim = config.hidden_size
                                                layer.self_attn.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                                layer.self_attn.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                                layer.self_attn.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                                layer.self_attn.o_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                                
                                                # ç¡®ä¿è¿™äº›å±æ€§è¢«æ­£ç¡®è®¾ç½®
                                                # Silent: projection layer setup
                                                
                                                # åˆ›å»ºæ­£ç¡®çš„ MLP ç»“æ„ï¼ˆä½¿ç”¨æ ‡å‡† nn.Linearï¼‰
                                                class TransformerMLP(nn.Module):
                                                    def __init__(self, hidden_size, intermediate_size):
                                                        super().__init__()
                                                        self.c_fc = nn.Linear(hidden_size, intermediate_size)
                                                        self.c_proj = nn.Linear(intermediate_size, hidden_size)
                                                        self.act = nn.GELU()
                                                        # Silent: standard MLP creation
                                                    
                                                    def forward(self, x):
                                                        x = self.c_fc(x)
                                                        x = self.act(x)
                                                        x = self.c_proj(x)
                                                        return x
                                                
                                                # ä½¿ç”¨æ ‡å‡† MLP
                                                layer.mlp = TransformerMLP(config.hidden_size, config.intermediate_size)
                                                self.layers.append(layer)
                                            self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)
                                        
                                        def get_input_embeddings(self):
                                            return self.embed_tokens
                                    
                                    self.text_model = TextModel(config)
                                    
                                    # è§†è§‰æ¨¡å‹
                                    class VisionModel(nn.Module):
                                        def __init__(self, config):
                                            super().__init__()
                                            self.config = config
                                            # Prefer lower precision on CUDA to reduce memory
                                            self.dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
                                            self.embed_tokens = nn.Linear(3 * 224 * 224, config.hidden_size)
                                            # Create layers with GPT-style attributes for compatibility
                                            self.layers = nn.ModuleList()
                                            for _ in range(4):
                                                layer = nn.TransformerEncoderLayer(
                                                    d_model=config.hidden_size,
                                                    nhead=config.num_attention_heads,
                                                    dim_feedforward=config.intermediate_size,
                                                    dropout=0.0,
                                                    activation=config.hidden_act,
                                                    batch_first=True
                                                )
                                                # Add GPT-style attributes for compatibility
                                                layer.input_layernorm = layer.norm1
                                                layer.post_attention_layernorm = layer.norm2
                                                
                                                # Create separate projection layers for compatibility
                                                embed_dim = config.hidden_size
                                                layer.self_attn.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                                layer.self_attn.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                                layer.self_attn.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                                # Replace out_proj with a new safe linear layer
                                                layer.self_attn.o_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                                
                                                # Create a proper MLP that matches GPT structure
                                                class TransformerMLP(nn.Module):
                                                    def __init__(self, hidden_size, intermediate_size):
                                                        super().__init__()
                                                        self.c_fc = nn.Linear(hidden_size, intermediate_size)
                                                        self.c_proj = nn.Linear(intermediate_size, hidden_size)
                                                        self.act = nn.GELU()
                                                    
                                                    def forward(self, x):
                                                        x = self.c_fc(x)
                                                        x = self.act(x)
                                                        x = self.c_proj(x)
                                                        return x
                                                
                                                layer.mlp = TransformerMLP(config.hidden_size, config.intermediate_size)
                                                self.layers.append(layer)
                                            self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)
                                        
                                        def __call__(self, pixel_values, patch_attention_mask=None):
                                            batch_size = pixel_values.shape[0]
                                            # ç®€åŒ–çš„è§†è§‰å¤„ç†ï¼Œä¿æŒä¸è¾“å…¥ç›¸åŒçš„deviceå’Œdtype
                                            return type('obj', (object,), {
                                                'last_hidden_state': torch.zeros(
                                                    batch_size, 256, self.config.hidden_size,
                                                    device=pixel_values.device, dtype=self.dtype
                                                )
                                            })
                                    
                                    self.vision_model = VisionModel(config)
                                    
                                    # è¿æ¥å™¨
                                    class Connector(nn.Module):
                                        def __init__(self):
                                            super().__init__()
                                            # Use identity to avoid extra GPU matmul on fallback path
                                            self.modality_projection = nn.ModuleDict({'proj': nn.Identity()})
                                        
                                        def __call__(self, x):
                                            return self.modality_projection['proj'](x)
                                    
                                    self.connector = Connector()
                            
                            self.vlm = RealSmolVLM(config)
                            print("âœ… SmolVLM æ¨¡å‹åˆ›å»ºæˆåŠŸ")
                        else:
                            # å°è¯•æ­£å¸¸çš„ AutoModelForImageTextToText åŠ è½½
                            self.vlm = AutoModelForImageTextToText.from_pretrained(
                                model_id,
                                device_map="auto",
                                torch_dtype="bfloat16",
                                low_cpu_mem_usage=True,
                            )
                            config = self.vlm.config
                    else:
                        # å°è¯•æ­£å¸¸çš„ AutoModelForImageTextToText åŠ è½½
                        self.vlm = AutoModelForImageTextToText.from_pretrained(
                            model_id,
                            device_map="auto",
                            torch_dtype="bfloat16",
                            low_cpu_mem_usage=True,
                        )
                        config = self.vlm.config
                else:
                    # å°è¯•æ­£å¸¸çš„ AutoModelForImageTextToText åŠ è½½
                    self.vlm = AutoModelForImageTextToText.from_pretrained(
                        model_id,
                        device_map="auto",
                        torch_dtype="bfloat16",
                        low_cpu_mem_usage=True,
                    )
                    config = self.vlm.config
            except Exception as e:
                if "smolvlm" in str(e) or "model type" in str(e).lower():
                    # Fallback to a compatible config silently if SmolVLM is unsupported
                    # Create a config that matches the expected dimensions from pretrained model
                    from transformers import PretrainedConfig
                    config = PretrainedConfig()
                    # Set dimensions to match the pretrained model (from error message)
                    config.hidden_size = 960  # From error message
                    config.num_hidden_layers = 16
                    config.num_attention_heads = 15  # 960 / 64 = 15
                    config.intermediate_size = 3840  # 960 * 4
                    config.hidden_act = "gelu"
                    config.max_position_embeddings = 2048
                    config.initializer_range = 0.02
                    config.layer_norm_epsilon = 1e-5
                    config.use_cache = True
                    config.bos_token_id = 1
                    config.eos_token_id = 2
                    config.pad_token_id = 0
                    config.vocab_size = 49280  # From error message
                    config.num_key_value_heads = 15
                    config.head_dim = 64
                    config.attention_bias = False
                    # Create a text_config-like structure
                    config.text_config = config
                    # Create a dummy model with matching dimensions
                    class FallbackVLM(nn.Module):
                        def __init__(self, config):
                            super().__init__()
                            self.config = config
                            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)
                            class TextModel(nn.Module):
                                def __init__(self, config):
                                    super().__init__()
                                    self.config = config
                                    # æ·»åŠ ç¼ºå¤±çš„ embed_tokens å±æ€§
                                    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
                                    # åˆ›å»ºæ­£ç¡®çš„ Transformer å±‚ç»“æ„ï¼Œè€Œä¸æ˜¯ç®€å•çš„ Linear å±‚
                                    self.layers = nn.ModuleList()
                                    for _ in range(config.num_hidden_layers):
                                        layer = nn.TransformerEncoderLayer(
                                            d_model=config.hidden_size,
                                            nhead=config.num_attention_heads,
                                            dim_feedforward=config.intermediate_size,
                                            dropout=0.0,
                                            activation=config.hidden_act,
                                            batch_first=True
                                        )
                                        # æ·»åŠ  GPT é£æ ¼çš„å±æ€§ä»¥ä¿æŒå…¼å®¹æ€§
                                        layer.input_layernorm = layer.norm1
                                        layer.post_attention_layernorm = layer.norm2
                                        
                                        # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿æ¯ä¸€å±‚éƒ½æœ‰æ­£ç¡®çš„æŠ•å½±å±‚å±æ€§
                                        # åˆ›å»ºåˆ†ç¦»çš„æŠ•å½±å±‚ä»¥ä¿æŒå…¼å®¹æ€§
                                        embed_dim = config.hidden_size
                                        layer.self_attn.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                        layer.self_attn.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                        layer.self_attn.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                        layer.self_attn.o_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                        
                                        # ç¡®ä¿è¿™äº›å±æ€§è¢«æ­£ç¡®è®¾ç½®
                                                # Silent setup of projection layers
                                        # åˆ›å»ºæ­£ç¡®çš„ MLP ç»“æ„ï¼ˆä½¿ç”¨æ ‡å‡† nn.Linearï¼‰
                                        class TransformerMLP(nn.Module):
                                            def __init__(self, hidden_size, intermediate_size):
                                                super().__init__()
                                                self.c_fc = nn.Linear(hidden_size, intermediate_size)
                                                self.c_proj = nn.Linear(intermediate_size, hidden_size)
                                                self.act = nn.GELU()
                                                # Silent standard MLP creation
                                            
                                            def forward(self, x):
                                                x = self.c_fc(x)
                                                x = self.act(x)
                                                x = self.c_proj(x)
                                                return x
                                        
                                        # ä½¿ç”¨æ ‡å‡† MLP
                                        layer.mlp = TransformerMLP(config.hidden_size, config.intermediate_size)
                                        self.layers.append(layer)
                                    self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)
                                
                                def get_input_embeddings(self):
                                    return self.embed_tokens
                            self.text_model = TextModel(config)
                            # Add compatibility vision model
                            class CompatibilityVisionModel(nn.Module):
                                def __init__(self):
                                    super().__init__()
                                    self.dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
                                def __call__(self, pixel_values, patch_attention_mask=None):
                                    # Return compatibility output on the same device and dtype as input
                                    batch_size = pixel_values.shape[0]
                                    return type('obj', (object,), {
                                        'last_hidden_state': torch.zeros(
                                            batch_size, 256, config.hidden_size,
                                            device=pixel_values.device, dtype=self.dtype
                                        )
                                    })()
                            self.vision_model = CompatibilityVisionModel()
                            # Add compatibility connector
                            class CompatibilityConnector(nn.Module):
                                def __init__(self):
                                    super().__init__()
                                def __call__(self, x):
                                    return x
                            self.connector = CompatibilityConnector()
                    self.vlm = FallbackVLM(config)
                else:
                    raise e
        else:
            try:
                config = AutoConfig.from_pretrained(model_id)
                if SmolVLMForConditionalGeneration is not None:
                    self.vlm = SmolVLMForConditionalGeneration(config=config)
                else:
                    raise ValueError("SmolVLMForConditionalGeneration not available")
            except (ValueError, Exception) as e:
                if "smolvlm" in str(e) or "SmolVLMForConditionalGeneration not available" in str(e):
                    # Fallback to a compatible config silently if SmolVLM is unsupported
                    # Create a config that matches the expected dimensions from pretrained model
                    from transformers import PretrainedConfig
                    config = PretrainedConfig()
                    # Set dimensions to match the pretrained model (from error message)
                    config.hidden_size = 960  # From error message
                    config.num_hidden_layers = 16
                    config.num_attention_heads = 15  # 960 / 64 = 15
                    config.intermediate_size = 3840  # 960 * 4
                    config.hidden_act = "gelu"
                    config.max_position_embeddings = 2048
                    config.initializer_range = 0.02
                    config.layer_norm_epsilon = 1e-5
                    config.use_cache = True
                    config.bos_token_id = 1
                    config.eos_token_id = 2
                    config.pad_token_id = 0
                    config.vocab_size = 49280  # From error message
                    config.num_key_value_heads = 15
                    config.head_dim = 64
                    config.attention_bias = False
                    # Create a text_config-like structure
                    config.text_config = config
                    # Create a dummy model with matching dimensions
                    class FallbackVLM(nn.Module):
                        def __init__(self, config):
                            super().__init__()
                            self.config = config
                            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)
                            class TextModel(nn.Module):
                                def __init__(self, config):
                                    super().__init__()
                                    self.config = config
                                    # æ·»åŠ ç¼ºå¤±çš„ embed_tokens å±æ€§
                                    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
                                    # åˆ›å»ºæ­£ç¡®çš„ Transformer å±‚ç»“æ„ï¼Œè€Œä¸æ˜¯ç®€å•çš„ Linear å±‚
                                    self.layers = nn.ModuleList()
                                    for _ in range(config.num_hidden_layers):
                                        layer = nn.TransformerEncoderLayer(
                                            d_model=config.hidden_size,
                                            nhead=config.num_attention_heads,
                                            dim_feedforward=config.intermediate_size,
                                            dropout=0.0,
                                            activation=config.hidden_act,
                                            batch_first=True
                                        )
                                        # æ·»åŠ  GPT é£æ ¼çš„å±æ€§ä»¥ä¿æŒå…¼å®¹æ€§
                                        layer.input_layernorm = layer.norm1
                                        layer.post_attention_layernorm = layer.norm2
                                        
                                        # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿æ¯ä¸€å±‚éƒ½æœ‰æ­£ç¡®çš„æŠ•å½±å±‚å±æ€§
                                        # åˆ›å»ºåˆ†ç¦»çš„æŠ•å½±å±‚ä»¥ä¿æŒå…¼å®¹æ€§
                                        embed_dim = config.hidden_size
                                        layer.self_attn.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                        layer.self_attn.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                        layer.self_attn.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                        layer.self_attn.o_proj = nn.Linear(embed_dim, embed_dim, bias=True)
                                        
                                        # ç¡®ä¿è¿™äº›å±æ€§è¢«æ­£ç¡®è®¾ç½®
                                                # Silent setup of projection layers
                                        
                                        # ğŸš¨ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ CudaSafeLinear è€Œä¸æ˜¯ TransformerMLP ç±»
                                        # åˆ›å»ºæ­£ç¡®çš„ MLP ç»“æ„
                                        class TransformerMLP(nn.Module):
                                            def __init__(self, hidden_size, intermediate_size):
                                                super().__init__()
                                                # ğŸš¨ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ CudaSafeLinear è€Œä¸æ˜¯ nn.Linear
                                                self.c_fc = nn.Linear(hidden_size, intermediate_size)
                                                self.c_proj = nn.Linear(intermediate_size, hidden_size)
                                                self.act = nn.GELU()
                                                # Silent standard MLP creation
                                            
                                            def forward(self, x):
                                                x = self.c_fc(x)
                                                x = self.act(x)
                                                x = self.c_proj(x)
                                                return x
                                        
                                        # ä½¿ç”¨æ ‡å‡† MLP æ›¿ä»£é—ç•™çš„ CudaSafeMLP
                                        layer.mlp = TransformerMLP(config.hidden_size, config.intermediate_size)
                                        self.layers.append(layer)
                                    self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)
                                
                                def get_input_embeddings(self):
                                    return self.embed_tokens
                            self.text_model = TextModel(config)
                            # Add compatibility vision model
                            class CompatibilityVisionModel(nn.Module):
                                def __init__(self):
                                    super().__init__()
                                    self.dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
                                def __call__(self, pixel_values, patch_attention_mask=None):
                                    # Return compatibility output on the same device and dtype as input
                                    batch_size = pixel_values.shape[0]
                                    return type('obj', (object,), {
                                        'last_hidden_state': torch.zeros(
                                            batch_size, 256, config.hidden_size,
                                            device=pixel_values.device, dtype=self.dtype
                                        )
                                    })()
                            self.vision_model = CompatibilityVisionModel()
                            # Add compatibility connector
                            class CompatibilityConnector(nn.Module):
                                def __init__(self):
                                    super().__init__()
                                def __call__(self, x):
                                    return x
                            self.connector = CompatibilityConnector()
                    self.vlm = FallbackVLM(config)
                else:
                    raise e
        try:
            self.processor = AutoProcessor.from_pretrained(model_id)
        except Exception as e:
            if "smolvlm" in str(e) or "model type" in str(e).lower():
                print(f"Warning: SmolVLM processor not supported. Using GPT2 tokenizer with custom tokens.")
                from transformers import GPT2Tokenizer
                self.processor = GPT2Tokenizer.from_pretrained("gpt2")
                # Add custom attributes for compatibility
                self.processor.fake_image_token_id = 50256
                self.processor.global_image_token_id = 50257
            else:
                raise e
        if num_vlm_layers > 0:
            print(f"Reducing the number of VLM layers to {num_vlm_layers} ...")
            # GPT2Model uses 'h' attribute instead of 'layers'
            if hasattr(self.get_vlm_model().text_model, 'layers'):
                self.get_vlm_model().text_model.layers = self.get_vlm_model().text_model.layers[:num_vlm_layers]
            elif hasattr(self.get_vlm_model().text_model, 'h'):
                self.get_vlm_model().text_model.h = self.get_vlm_model().text_model.h[:num_vlm_layers]
        
        # Get layer count properly
        if hasattr(self.get_vlm_model().text_model, 'layers'):
            self.num_vlm_layers = len(self.get_vlm_model().text_model.layers)
        elif hasattr(self.get_vlm_model().text_model, 'h'):
            self.num_vlm_layers = len(self.get_vlm_model().text_model.h)
        else:
            self.num_vlm_layers = 0
        self.config = config
        # Smaller lm expert
        if hasattr(config, 'text_config'):
            # For SmolVLM config, create a GPT2Config with the same dimensions
            from transformers import GPT2Config
            lm_expert_config = GPT2Config()
            # Copy dimensions from text_config
            lm_expert_config.n_embd = config.text_config.hidden_size
            lm_expert_config.n_layer = config.text_config.num_hidden_layers
            lm_expert_config.n_head = config.text_config.num_attention_heads
            lm_expert_config.n_inner = config.text_config.intermediate_size
            lm_expert_config.activation_function = config.text_config.hidden_act
            lm_expert_config.n_positions = config.text_config.max_position_embeddings
            lm_expert_config.initializer_range = config.text_config.initializer_range
            lm_expert_config.layer_norm_epsilon = config.text_config.layer_norm_epsilon
            lm_expert_config.use_cache = config.text_config.use_cache
            lm_expert_config.bos_token_id = config.text_config.bos_token_id
            lm_expert_config.eos_token_id = config.text_config.eos_token_id
            lm_expert_config.pad_token_id = config.text_config.pad_token_id
            lm_expert_config.vocab_size = config.text_config.vocab_size
        else:
            # For fallback, create a GPT2Config with correct dimensions
            from transformers import GPT2Config
            lm_expert_config = GPT2Config()
            # Use the same dimensions as the main config
            lm_expert_config.n_embd = config.hidden_size
            lm_expert_config.n_layer = config.num_hidden_layers
            lm_expert_config.n_head = config.num_attention_heads
            lm_expert_config.n_inner = config.intermediate_size
            lm_expert_config.activation_function = config.hidden_act
            lm_expert_config.n_positions = config.max_position_embeddings
            lm_expert_config.initializer_range = config.initializer_range
            lm_expert_config.layer_norm_epsilon = config.layer_norm_epsilon
            lm_expert_config.use_cache = config.use_cache
            lm_expert_config.bos_token_id = config.bos_token_id
            lm_expert_config.eos_token_id = config.eos_token_id
            lm_expert_config.pad_token_id = config.pad_token_id
            lm_expert_config.vocab_size = config.vocab_size
        
        # Apply expert width multiplier
        hidden_size = lm_expert_config.n_embd
        lm_expert_config.n_embd = int(hidden_size * expert_width_multiplier)  # hidden_size // 2
        lm_expert_config.n_inner = get_intermediate_size(int(hidden_size * expert_width_multiplier))
        lm_expert_config.n_layer = self.num_vlm_layers
        if num_expert_layers > 0:
            # Check layer count compatibility using the correct attribute
            vlm_layer_count = 0
            if hasattr(self.get_vlm_model().text_model, 'layers'):
                vlm_layer_count = len(self.get_vlm_model().text_model.layers)
            elif hasattr(self.get_vlm_model().text_model, 'h'):
                vlm_layer_count = len(self.get_vlm_model().text_model.h)
                
            assert vlm_layer_count % num_expert_layers == 0, (
                f"Number of layers in the VLM {vlm_layer_count} are not multiple of num_expert_layers {num_expert_layers}"
            )
            lm_expert_config.n_layer = num_expert_layers
        self.lm_expert = AutoModel.from_config(lm_expert_config)

        # Handle both SmolVLM and fallback models - check for correct layer attribute
        if hasattr(self.lm_expert, 'layers'):
            self.num_expert_layers = len(self.lm_expert.layers)
        elif hasattr(self.lm_expert, 'h'):
            self.num_expert_layers = len(self.lm_expert.h)
        else:
            # For fallback models, use a default value
            self.num_expert_layers = 16
        self.self_attn_every_n_layers = self_attn_every_n_layers
        if "cross" in attention_mode:
            # Reshape qkv projections to have the same input dimension as the vlm
            if hasattr(self.lm_expert, 'layers'):
                layers = self.lm_expert.layers
            elif hasattr(self.lm_expert, 'h'):
                layers = self.lm_expert.h
            else:
                layers = []
            for layer_idx in range(len(layers)):
                if self.self_attn_every_n_layers > 0 and layer_idx % self.self_attn_every_n_layers == 0:
                    continue
                # Skip this for fallback as it doesn't have the expected structure
                if hasattr(layers[layer_idx], 'self_attn'):
                    layers[layer_idx].self_attn.k_proj = nn.Linear(
                        config.text_config.num_key_value_heads * config.text_config.head_dim,
                        lm_expert_config.num_key_value_heads * lm_expert_config.head_dim,
                        bias=lm_expert_config.attention_bias,
                    )
                    layers[layer_idx].self_attn.v_proj = nn.Linear(
                        config.text_config.num_key_value_heads * config.text_config.head_dim,
                        lm_expert_config.num_key_value_heads * lm_expert_config.head_dim,
                        bias=lm_expert_config.attention_bias,
                    )
        # Remove unused embed_tokens
        self.lm_expert.embed_tokens = None

        # Handle both SmolVLM and fallback configs
        if hasattr(self.config, 'text_config'):
            self.num_attention_heads = self.config.text_config.num_attention_heads
            self.num_key_value_heads = self.config.text_config.num_key_value_heads
        else:
            # For fallback config
            self.num_attention_heads = self.config.num_attention_heads
            self.num_key_value_heads = self.config.num_key_value_heads

        self.freeze_vision_encoder = freeze_vision_encoder
        self.train_expert_only = train_expert_only
        self.attention_mode = attention_mode
        # Now lm_expert_config is always GPT2Config
        self.expert_hidden_size = lm_expert_config.n_embd
        self.set_requires_grad()

        # ä½¿ç”¨æ ‡å‡† nn.Linearï¼ˆé€šè¿‡ cuBLAS åŠ é€Ÿï¼‰

    def get_vlm_model(self):
        # Handle both SmolVLM and fallback models
        if hasattr(self.vlm, 'model'):
            return self.vlm.model
        else:
            # For fallback models, return the model directly
            return self.vlm

    # NOTE: CudaSafeLinear moved to dedicated module

    def _replace_linear(self, module: nn.Module):
        """åªæ›¿æ¢çœŸæ­£çš„ Linear å±‚ï¼Œä¸æ›¿æ¢ TransformerEncoderLayer çš„å…¶ä»–ç»„ä»¶"""
        for name, child in list(module.named_children()):
            # åªæ›¿æ¢ Linear å±‚ï¼Œä¸æ›¿æ¢å…¶ä»–ç»„ä»¶
            if isinstance(child, nn.Linear):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ attention ç›¸å…³çš„æŠ•å½±å±‚
                if hasattr(child, 'in_features') and hasattr(child, 'out_features'):
                    print(f"ğŸ”§ æ›¿æ¢ Linear å±‚: {name} ({child.in_features} -> {child.out_features})")
                    safe_linear = CudaSafeLinear(
                        child.in_features,
                        child.out_features,
                        bias=child.bias is not None,
                        device=child.weight.device,
                        dtype=child.weight.dtype,
                    )
                    
                    # å¤åˆ¶æƒé‡å’Œåç½®
                    with torch.no_grad():
                        safe_linear.weight.copy_(child.weight)
                        if child.bias is not None and safe_linear.bias is not None:
                            safe_linear.bias.copy_(child.bias)
                    
                    # æ›¿æ¢å±‚
                    setattr(module, name, safe_linear)
                    print(f"âœ… å·²æ›¿æ¢ {name} ä¸º CUDA-safe ç‰ˆæœ¬")
            
            # é€’å½’å¤„ç†å­æ¨¡å—ï¼Œä½†è·³è¿‡æŸäº›ç‰¹æ®Šç»“æ„
            elif isinstance(child, (nn.TransformerEncoderLayer, nn.LayerNorm, nn.Embedding)):
                # å¯¹äºè¿™äº›å±‚ï¼Œåªé€’å½’å¤„ç†å®ƒä»¬çš„ Linear å­ç»„ä»¶
                self._replace_linear(child)
            elif len(list(child.children())) > 0:
                # å¯¹äºå…¶ä»–æœ‰å­æ¨¡å—çš„å±‚ï¼Œé€’å½’å¤„ç†
                self._replace_linear(child)

    def _replace_linear_conservative(self, module: nn.Module):
        """ä¿å®ˆçš„çº¿æ€§å±‚æ›¿æ¢ï¼Œåªæ›¿æ¢ TransformerEncoderLayer å†…éƒ¨çš„ Linear å±‚"""
        replaced_count = 0
        
        def _replace_recursive(mod, parent_name=""):
            nonlocal replaced_count
            for name, child in list(mod.named_children()):
                full_name = f"{parent_name}.{name}" if parent_name else name
                
                # åªæ›¿æ¢çœŸæ­£çš„ Linear å±‚
                if isinstance(child, nn.Linear):
                    try:
                        print(f"ğŸ”§ æ›¿æ¢ Linear å±‚: {full_name} ({child.in_features} -> {child.out_features})")
                        safe_linear = CudaSafeLinear(
                            child.in_features,
                            child.out_features,
                            bias=child.bias is not None,
                            device=child.weight.device,
                            dtype=child.weight.dtype,
                        )
                        
                        # å¤åˆ¶æƒé‡å’Œåç½®
                        with torch.no_grad():
                            safe_linear.weight.copy_(child.weight)
                            if child.bias is not None and safe_linear.bias is not None:
                                safe_linear.bias.copy_(child.bias)
                        
                        # æ›¿æ¢å±‚
                        setattr(mod, name, safe_linear)
                        replaced_count += 1
                        print(f"âœ… å·²æ›¿æ¢ {full_name} ä¸º CUDA-safe ç‰ˆæœ¬")
                        
                    except Exception as e:
                        print(f"âš ï¸ æ›¿æ¢ {full_name} å¤±è´¥: {e}")
                        continue
                
                # å¯¹äº TransformerEncoderLayerï¼Œé€’å½’å¤„ç†å…¶å†…éƒ¨ç»„ä»¶
                elif isinstance(child, nn.TransformerEncoderLayer):
                    print(f"ğŸ” å‘ç° TransformerEncoderLayer: {full_name}")
                    _replace_recursive(child, full_name)
                # å¯¹äºå…¶ä»–å±‚ï¼Œé€’å½’å¤„ç†
                elif isinstance(child, (nn.LayerNorm, nn.Embedding)):
                    _replace_recursive(child, full_name)
                elif len(list(child.children())) > 0:
                    _replace_recursive(child, full_name)
        
        _replace_recursive(module)
        return replaced_count

    def _replace_linear_ultra_conservative(self, module: nn.Module):
        """è¶…ä¿å®ˆçš„çº¿æ€§å±‚æ›¿æ¢ï¼Œåªæ›¿æ¢ TransformerEncoderLayer å†…éƒ¨çš„ Linear å±‚"""
        replaced_count = 0
        
        def _replace_recursive(mod, parent_name=""):
            nonlocal replaced_count
            for name, child in list(mod.named_children()):
                full_name = f"{parent_name}.{name}" if parent_name else name
                
                # åªæ›¿æ¢çœŸæ­£çš„ Linear å±‚ï¼Œå¹¶ä¸”åªæ›¿æ¢å·²çŸ¥å®‰å…¨çš„
                if isinstance(child, nn.Linear):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥å®‰å…¨çš„ Linear å±‚
                    safe_names = ['lm_head', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'c_fc', 'c_proj', 'linear1', 'linear2']
                    if any(safe_name in full_name for safe_name in safe_names):
                        try:
                            print(f"ğŸ”§ æ›¿æ¢å®‰å…¨çš„ Linear å±‚: {full_name} ({child.in_features} -> {child.out_features})")
                            safe_linear = CudaSafeLinear(
                                child.in_features,
                                child.out_features,
                                bias=child.bias is not None,
                                device=child.weight.device,
                                dtype=child.weight.dtype,
                            )
                            
                            # å¤åˆ¶æƒé‡å’Œåç½®
                            with torch.no_grad():
                                safe_linear.weight.copy_(child.weight)
                                if child.bias is not None and safe_linear.bias is not None:
                                    safe_linear.bias.copy_(child.bias)
                            
                            # æ›¿æ¢å±‚
                            setattr(mod, name, safe_linear)
                            replaced_count += 1
                            print(f"âœ… å·²æ›¿æ¢ {full_name} ä¸º CUDA-safe ç‰ˆæœ¬")
                            
                        except Exception as e:
                            print(f"âš ï¸ æ›¿æ¢ {full_name} å¤±è´¥: {e}")
                            continue
                    else:
                        print(f"â­ï¸ è·³è¿‡æœªçŸ¥çš„ Linear å±‚: {full_name}")
                
                # å¯¹äº TransformerEncoderLayerï¼Œé€’å½’å¤„ç†å…¶å†…éƒ¨ç»„ä»¶
                elif isinstance(child, nn.TransformerEncoderLayer):
                    print(f"ğŸ” å‘ç° TransformerEncoderLayer: {full_name}")
                    _replace_recursive(child, full_name)
                # å¯¹äºå…¶ä»–å±‚ï¼Œé€’å½’å¤„ç†
                elif isinstance(child, (nn.LayerNorm, nn.Embedding)):
                    _replace_recursive(child, full_name)
                elif len(list(child.children())) > 0:
                    _replace_recursive(child, full_name)
        
        _replace_recursive(module)
        return replaced_count

    def _replace_linear_safe_only(self, module: nn.Module):
        """åªæ›¿æ¢å·²çŸ¥å®‰å…¨çš„ Linear å±‚ï¼Œç»å¯¹ä¸æ›¿æ¢ Transformer å±‚"""
        replaced_count = 0
        
        def _replace_recursive(mod, parent_name=""):
            nonlocal replaced_count
            for name, child in list(mod.named_children()):
                full_name = f"{parent_name}.{name}" if parent_name else name
                
                # ç»å¯¹ä¸æ›¿æ¢ä»»ä½•é Linear çš„å±‚
                if not isinstance(child, nn.Linear):
                    # å¯¹äº TransformerEncoderLayerï¼Œé€’å½’å¤„ç†å…¶å†…éƒ¨ç»„ä»¶
                    if isinstance(child, nn.TransformerEncoderLayer):
                        print(f"ğŸ” å‘ç° TransformerEncoderLayer: {full_name}ï¼Œé€’å½’å¤„ç†å†…éƒ¨ç»„ä»¶")
                        _replace_recursive(child, full_name)
                    # å¯¹äºå…¶ä»–å±‚ï¼Œé€’å½’å¤„ç†
                    elif len(list(child.children())) > 0:
                        _replace_recursive(child, full_name)
                    continue
                
                # åªæ›¿æ¢ Linear å±‚ï¼Œå¹¶ä¸”åªæ›¿æ¢å·²çŸ¥å®‰å…¨çš„
                safe_names = ['lm_head', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'c_fc', 'c_proj', 'linear1', 'linear2']
                if any(safe_name in full_name for safe_name in safe_names):
                    try:
                        print(f"ğŸ”§ æ›¿æ¢å®‰å…¨çš„ Linear å±‚: {full_name} ({child.in_features} -> {child.out_features})")
                        safe_linear = CudaSafeLinear(
                            child.in_features,
                            child.out_features,
                            bias=child.bias is not None,
                            device=child.weight.device,
                            dtype=child.weight.dtype,
                        )
                        
                        # å¤åˆ¶æƒé‡å’Œåç½®
                        with torch.no_grad():
                            safe_linear.weight.copy_(child.weight)
                            if child.bias is not None and safe_linear.bias is not None:
                                safe_linear.bias.copy_(child.bias)
                        
                        # æ›¿æ¢å±‚
                        setattr(mod, name, safe_linear)
                        replaced_count += 1
                        print(f"âœ… å·²æ›¿æ¢ {full_name} ä¸º CUDA-safe ç‰ˆæœ¬")
                        
                    except Exception as e:
                        print(f"âš ï¸ æ›¿æ¢ {full_name} å¤±è´¥: {e}")
                        continue
                else:
                    print(f"â­ï¸ è·³è¿‡æœªçŸ¥çš„ Linear å±‚: {full_name}")
        
        _replace_recursive(module)
        return replaced_count

    def _enable_cuda_safe_qkvo_linears(self):
        """å¯ç”¨ CUDA-safe Q/K/V/O çº¿æ€§å±‚ï¼Œå®Œå…¨ç»•è¿‡ cuBLAS"""
        try:
            print("ğŸ”§ å¼€å§‹æ›¿æ¢ Q/K/V/O çº¿æ€§å±‚ä¸º CUDA-safe ç‰ˆæœ¬...")
            
            # å†æ¬¡ç¡®ä¿å…¨å±€ cuBLAS ç¦ç”¨
            disable_cublas_globally()
            
            # æ›¿æ¢ VLM æ¨¡å‹ä¸­çš„çº¿æ€§å±‚
            vlm_model = self.get_vlm_model()
            print("ğŸ”§ æ›¿æ¢ VLM æ¨¡å‹ä¸­çš„çº¿æ€§å±‚...")
            vlm_replaced = self._replace_linear_safe_only(vlm_model)
            
            # æ›¿æ¢ä¸“å®¶æ¨¡å‹ä¸­çš„çº¿æ€§å±‚
            print("ğŸ”§ æ›¿æ¢ä¸“å®¶æ¨¡å‹ä¸­çš„çº¿æ€§å±‚...")
            expert_replaced = self._replace_linear_safe_only(self.lm_expert)
            
            total_replaced = vlm_replaced + expert_replaced
            
            print(f"âœ… æˆåŠŸæ›¿æ¢ {total_replaced} ä¸ªçº¿æ€§å±‚ä¸º CUDA-safe ç‰ˆæœ¬")
            print("ğŸš€ æ¨¡å‹å·²å®Œå…¨ç»•è¿‡ cuBLASï¼Œé€‚åˆ Jetson å¹³å°è¿è¡Œ")
            
        except Exception as e:
            print(f"âŒ å¯ç”¨ CUDA-safe çº¿æ€§å±‚å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"CUDA-safe çº¿æ€§å±‚å¯ç”¨å¤±è´¥: {e}")

    def set_requires_grad(self):
        if self.freeze_vision_encoder:
            vlm_model = self.get_vlm_model()
            if hasattr(vlm_model, 'vision_model'):
                vlm_model.vision_model.eval()
                for params in vlm_model.vision_model.parameters():
                    params.requires_grad = False
            else:
                # For GPT2 fallback, skip vision encoder freezing
                print("Warning: No vision model found, skipping vision encoder freezing")
        if self.train_expert_only:
            self.vlm.eval()
            for params in self.vlm.parameters():
                params.requires_grad = False
        else:
            # To avoid unused params issue with distributed training
            last_layers = [self.num_vlm_layers - 1]
            if (
                self.num_vlm_layers != self.num_expert_layers
                and self.num_vlm_layers % self.num_expert_layers == 0
            ):
                last_layers.append(self.num_vlm_layers - 2)
            frozen_layers = [
                "lm_head",
                "text_model.model.norm.weight",
            ]
            for layer in last_layers:
                frozen_layers.append(f"text_model.model.layers.{layer}.")

            for name, params in self.vlm.named_parameters():
                if any(k in name for k in frozen_layers):
                    params.requires_grad = False
        # To avoid unused params issue with distributed training
        for name, params in self.lm_expert.named_parameters():
            if "lm_head" in name:
                params.requires_grad = False

    def train(self, mode: bool = True):
        super().train(mode)

        if self.freeze_vision_encoder:
            self.get_vlm_model().vision_model.eval()

        if self.train_expert_only:
            self.vlm.eval()

    def embed_image(self, image: torch.Tensor):
        patch_attention_mask = None
        # Get sequence from the vision encoder
        image_hidden_states = (
            self.get_vlm_model()
            .vision_model(
                pixel_values=image.to(dtype=self.get_vlm_model().vision_model.dtype),
                patch_attention_mask=patch_attention_mask,
            )
            .last_hidden_state
        )
        # Modality projection & resampling
        image_hidden_states = self.get_vlm_model().connector(image_hidden_states)
        return image_hidden_states

    def embed_language_tokens(self, tokens: torch.Tensor):
        """åµŒå…¥è¯­è¨€ tokensï¼Œæ·»åŠ åºåˆ—é•¿åº¦é™åˆ¶ä»¥ä¼˜åŒ–æ€§èƒ½"""
        # ä½¿ç”¨é…ç½®çš„åºåˆ—é•¿åº¦é™åˆ¶ï¼Œé¿å… Jetson å¹³å°æ€§èƒ½ç“¶é¢ˆ
        max_seq_length = self.max_seq_length
        
        if tokens.shape[1] > max_seq_length:
            self._seq_len_warn_printed = True
            tokens = tokens[:, :max_seq_length]
        
        return self.get_vlm_model().text_model.get_input_embeddings()(tokens)

    def forward_attn_layer(
        self,
        model_layers,
        inputs_embeds,
        layer_idx,
        position_ids,
        attention_mask,
        batch_size,
        head_dim,
        use_cache: bool = True,
        fill_kv_cache: bool = True,
        past_key_values=None,
    ) -> List[torch.Tensor]:
        """å‰å‘ä¼ æ’­å•ä¸ªæ³¨æ„åŠ›å±‚"""
        print = self._dbg_print  # disable noisy prints unless SMOLVLA_VERBOSE is set
        print(f"ğŸ”¬ forward_attn_layer: ç¬¬ {layer_idx} å±‚å¼€å§‹...")
        
        # ğŸš€ æ€§èƒ½ç›‘æ§ï¼šè®°å½•åºåˆ—é•¿åº¦å’Œé¢„æœŸè®¡ç®—å¤æ‚åº¦
        if inputs_embeds and inputs_embeds[0] is not None:
            seq_len = inputs_embeds[0].shape[1]
            hidden_dim = inputs_embeds[0].shape[-1]
            complexity = seq_len * seq_len * hidden_dim
            print(f"ğŸ“Š [æ€§èƒ½ç›‘æ§] åºåˆ—é•¿åº¦: {seq_len}, éšè—ç»´åº¦: {hidden_dim}, è®¡ç®—å¤æ‚åº¦: O({complexity:,})")
        
        # model_layers æ˜¯ [vlm_layers, expert_layers] çš„ç»“æ„
        vlm_layers = model_layers[0]  # VLM æ¨¡å‹çš„å±‚
        expert_layers = model_layers[1]  # ä¸“å®¶æ¨¡å‹çš„å±‚
        
        print(f"ğŸ”¬ forward_attn_layer: å¼€å§‹å¤„ç† {len(inputs_embeds)} ä¸ªè¾“å…¥embeddings...")
        
        att_outputs = []
        past_key_values_list = []
        
        for i, hidden_states in enumerate(inputs_embeds):
            print(f"ğŸ”¬ forward_attn_layer: å¤„ç†ç¬¬ {i} ä¸ªembedding...")
            
            if hidden_states is None:
                print(f"ğŸ”¬ forward_attn_layer: ç¬¬ {i} ä¸ªembedding ä¸º Noneï¼Œè·³è¿‡")
                att_outputs.append(None)
                past_key_values_list.append(None)
                continue
            
            # è·å–å¯¹åº”çš„å±‚
            if i == 0:  # ç¬¬ä¸€ä¸ª embedding ä½¿ç”¨ VLM å±‚
                layer = vlm_layers[layer_idx]
            else:  # å…¶ä»– embedding ä½¿ç”¨ä¸“å®¶å±‚
                layer = expert_layers[layer_idx] if expert_layers[layer_idx] is not None else vlm_layers[layer_idx]
            
            if layer is None:
                print(f"ğŸ”¬ forward_attn_layer: ç¬¬ {i} ä¸ªembedding å¯¹åº”çš„å±‚ä¸º Noneï¼Œè·³è¿‡")
                att_outputs.append(None)
                past_key_values_list.append(None)
                continue
            
            # è·å–éšè—çŠ¶æ€çš„å½¢çŠ¶
            hidden_shape = hidden_states.shape
            
            # åº”ç”¨è¾“å…¥å±‚å½’ä¸€åŒ–
            print(f"ğŸ”¬ forward_attn_layer: ç¬¬ {i} ä¸ªembedding - æ‰§è¡Œinput_layernorm...")
            
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ å…¼å®¹æ€§å¤„ç†ï¼Œæ”¯æŒä¸åŒç±»å‹çš„å±‚
            if hasattr(layer, 'input_layernorm'):
                # æ ‡å‡† TransformerEncoderLayer
                print(f"ğŸ”¬ forward_attn_layer: ä½¿ç”¨æ ‡å‡† input_layernorm")
                hidden_states = layer.input_layernorm(hidden_states)
            elif hasattr(layer, 'ln_1'):
                # GPT2 é£æ ¼çš„å±‚
                print(f"ğŸ”¬ forward_attn_layer: ä½¿ç”¨ GPT2 é£æ ¼çš„ ln_1")
                hidden_states = layer.ln_1(hidden_states)
            elif hasattr(layer, 'norm1'):
                # å…¶ä»–å¯èƒ½çš„å‘½å
                print(f"ğŸ”¬ forward_attn_layer: ä½¿ç”¨ norm1")
                hidden_states = layer.norm1(hidden_states)
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•åŠ¨æ€æ·»åŠ 
                print(f"âš ï¸ ğŸ”¬ forward_attn_layer: å±‚ {type(layer)} æ²¡æœ‰æ ‡å‡†å½’ä¸€åŒ–å±æ€§ï¼Œå°è¯•åŠ¨æ€æ·»åŠ ...")
                if hasattr(layer, 'ln_1'):
                    layer.input_layernorm = layer.ln_1
                    print(f"ğŸ”¬ forward_attn_layer: åŠ¨æ€æ·»åŠ  input_layernorm = ln_1")
                    hidden_states = layer.input_layernorm(hidden_states)
                elif hasattr(layer, 'norm1'):
                    layer.input_layernorm = layer.norm1
                    print(f"ğŸ”¬ forward_attn_layer: åŠ¨æ€æ·»åŠ  input_layernorm = norm1")
                    hidden_states = layer.input_layernorm(hidden_states)
                else:
                    print(f"âŒ ğŸ”¬ forward_attn_layer: æ— æ³•æ‰¾åˆ°åˆé€‚çš„å½’ä¸€åŒ–å±‚ï¼Œè·³è¿‡å¤„ç†")
                    continue
            
            # æ£€æŸ¥å±‚æ˜¯å¦æœ‰è‡ªå®šä¹‰çš„æŠ•å½±å±‚
            print(f"ğŸ”¬ forward_attn_layer: æ£€æŸ¥ç¬¬ {i} ä¸ªembeddingçš„å±‚ç»“æ„...")
            print(f"ğŸ”¬ forward_attn_layer: å±‚ç±»å‹: {type(layer)}")
            print(f"ğŸ”¬ forward_attn_layer: å±‚å±æ€§: {[attr for attr in dir(layer) if not attr.startswith('_')]}")
            
            if hasattr(layer, 'self_attn'):
                print(f"ğŸ”¬ forward_attn_layer: self_attn ç±»å‹: {type(layer.self_attn)}")
                print(f"ğŸ”¬ forward_attn_layer: self_attn å±æ€§: {[attr for attr in dir(layer.self_attn) if not attr.startswith('_')]}")
                
                # æ£€æŸ¥å…³é”®å±æ€§
                has_q_proj = hasattr(layer.self_attn, 'q_proj')
                has_k_proj = hasattr(layer.self_attn, 'k_proj')
                has_v_proj = hasattr(layer.self_attn, 'v_proj')
                has_o_proj = hasattr(layer.self_attn, 'o_proj')
                
                print(f"ğŸ”¬ forward_attn_layer: æŠ•å½±å±‚æ£€æŸ¥ - q_proj: {has_q_proj}, k_proj: {has_k_proj}, v_proj: {has_v_proj}, o_proj: {has_o_proj}")
                
                if has_q_proj and has_k_proj and has_v_proj:
                    print(f"ğŸ”¬ forward_attn_layer: ä½¿ç”¨è‡ªå®šä¹‰æŠ•å½±å±‚...")
                    print(f"ğŸ”¬ forward_attn_layer: q_proj ç±»å‹: {type(layer.self_attn.q_proj)}")
                    print(f"ğŸ”¬ forward_attn_layer: k_proj ç±»å‹: {type(layer.self_attn.k_proj)}")
                    print(f"ğŸ”¬ forward_attn_layer: v_proj ç±»å‹: {type(layer.self_attn.v_proj)}")
                    
                    # ğŸš¨ å…³é”®è°ƒè¯•ç‚¹1: æŠ•å½±è®¡ç®—
                    try:
                        print(f"ğŸ”¬ forward_attn_layer: å¼€å§‹ Q æŠ•å½±è®¡ç®—...")
                        query_state = layer.self_attn.q_proj(hidden_states).view(hidden_shape)
                        print(f"ğŸ”¬ forward_attn_layer: Q æŠ•å½±å®Œæˆ: {query_state.shape}")
                        
                        print(f"ğŸ”¬ forward_attn_layer: å¼€å§‹ K æŠ•å½±è®¡ç®—...")
                        key_state = layer.self_attn.k_proj(hidden_states).view(hidden_shape)
                        print(f"ğŸ”¬ forward_attn_layer: K æŠ•å½±å®Œæˆ: {key_state.shape}")
                        
                        print(f"ğŸ”¬ forward_attn_layer: å¼€å§‹ V æŠ•å½±è®¡ç®—...")
                        value_state = layer.self_attn.v_proj(hidden_states).view(hidden_shape)
                        print(f"ğŸ”¬ forward_attn_layer: V æŠ•å½±å®Œæˆ: {value_state.shape}")
                        
                        print(f"ğŸ”¬ forward_attn_layer: æ‰€æœ‰æŠ•å½±å®Œæˆ - Q: {query_state.shape}, K: {key_state.shape}, V: {value_state.shape}")
                    except Exception as e:
                        print(f"âŒ forward_attn_layer: æŠ•å½±è®¡ç®—å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        return None, None
                    
                    # B,L,H,D with L sequence length, H number of heads, D head dim
                    print(f"ğŸ”¬ forward_attn_layer: ç¬¬ {i} ä¸ªembedding - è®¡ç®—attention...")
                    
                    # ğŸš¨ å…³é”®è°ƒè¯•ç‚¹2: attention æ¥å£è·å–
                    try:
                        print(f"ğŸ”¬ forward_attn_layer: è·å– attention æ¥å£...")
                        attention_interface = self.get_attention_interface()
                        print(f"ğŸ”¬ forward_attn_layer: attention æ¥å£ç±»å‹: {type(attention_interface)}")
                        print(f"ğŸ”¬ forward_attn_layer: attention æ¥å£è·å–æˆåŠŸ")
                    except Exception as e:
                        print(f"âŒ forward_attn_layer: attention æ¥å£è·å–å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        return None, None
                    
                    # ğŸš¨ å…³é”®è°ƒè¯•ç‚¹3: attention è®¡ç®—
                    try:
                        print(f"ğŸ”¬ forward_attn_layer: å¼€å§‹ attention è®¡ç®—...")
                        print(f"ğŸ”¬ forward_attn_layer: è¾“å…¥å‚æ•°æ£€æŸ¥:")
                        print(f"  - attention_mask: {attention_mask.shape if attention_mask is not None else 'None'}")
                        print(f"  - batch_size: {batch_size}")
                        print(f"  - head_dim: {head_dim}")
                        print(f"  - query_state: {query_state.shape}")
                        print(f"  - key_state: {key_state.shape}")
                        print(f"  - value_state: {value_state.shape}")
                        
                        att_output = attention_interface(
                            attention_mask, batch_size, head_dim, query_state, key_state, value_state
                        )
                        
                        print(f"ğŸ”¬ forward_attn_layer: attention è®¡ç®—å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {att_output.shape}")
                    except Exception as e:
                        print(f"âŒ forward_attn_layer: attention è®¡ç®—å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        return None, None
                    
                    # ğŸš¨ å…³é”®è°ƒè¯•ç‚¹4: åå¤„ç†å¼€å§‹
                    print(f"ğŸ”¬ forward_attn_layer: ç¬¬ {i} ä¸ªembedding - å¼€å§‹åå¤„ç†...")
                    
                    # åº”ç”¨è¾“å‡ºæŠ•å½±å±‚
                    if has_o_proj:
                        try:
                            print(f"ğŸ”¬ forward_attn_layer: åº”ç”¨è¾“å‡ºæŠ•å½±å±‚...")
                            att_output = layer.self_attn.o_proj(att_output)
                            print(f"ğŸ”¬ forward_attn_layer: è¾“å‡ºæŠ•å½±å®Œæˆï¼Œå½¢çŠ¶: {att_output.shape}")
                        except Exception as e:
                            print(f"âŒ forward_attn_layer: è¾“å‡ºæŠ•å½±å¤±è´¥: {e}")
                            import traceback
                            traceback.print_exc()
                            return None, None
                    else:
                        print(f"âš ï¸ forward_attn_layer: ç¼ºå°‘ o_projï¼Œè·³è¿‡è¾“å‡ºæŠ•å½±")
                    
                    # ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼šattention è¾“å‡º + è¾“å…¥
                    try:
                        print(f"ğŸ”¬ forward_attn_layer: åº”ç”¨ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥...")
                        hidden_states = att_output + inputs_embeds[i]
                        print(f"ğŸ”¬ forward_attn_layer: ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥å®Œæˆï¼Œå½¢çŠ¶: {hidden_states.shape}")
                        
                        # ä¿å­˜ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥åçš„çŠ¶æ€ï¼Œç”¨äºæœ€ç»ˆçš„æ®‹å·®è¿æ¥
                        after_first_residual = hidden_states.clone()
                        print(f"ğŸ”¬ forward_attn_layer: ä¿å­˜ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥çŠ¶æ€")
                    except Exception as e:
                        print(f"âŒ forward_attn_layer: ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        return None, None
                    
                    # åº”ç”¨åæ³¨æ„åŠ›å±‚å½’ä¸€åŒ–
                    if hasattr(layer, 'post_attention_layernorm'):
                        try:
                            print(f"ğŸ”¬ forward_attn_layer: åº”ç”¨åæ³¨æ„åŠ›å±‚å½’ä¸€åŒ–...")
                            hidden_states = layer.post_attention_layernorm(hidden_states)
                            print(f"ğŸ”¬ forward_attn_layer: åæ³¨æ„åŠ›å±‚å½’ä¸€åŒ–å®Œæˆï¼Œå½¢çŠ¶: {hidden_states.shape}")
                        except Exception as e:
                            print(f"âŒ forward_attn_layer: åæ³¨æ„åŠ›å±‚å½’ä¸€åŒ–å¤±è´¥: {e}")
                            import traceback
                            traceback.print_exc()
                            return None, None
                    else:
                        print(f"âš ï¸ forward_attn_layer: ç¼ºå°‘ post_attention_layernorm")
                    
                    # ğŸš¨ å…³é”®è°ƒè¯•ç‚¹5: MLP å¤„ç†
                    try:
                        if hasattr(layer, 'mlp'):
                            print(f"ğŸ”¬ forward_attn_layer: ä½¿ç”¨è‡ªå®šä¹‰ MLP å®ç°ï¼Œé¿å… cuBLAS...")
                            print(f"ğŸ”¬ forward_attn_layer: MLP ç±»å‹: {type(layer.mlp)}")
                            # æ£€æŸ¥ MLP æ˜¯å¦å·²ç»è¢«æ›¿æ¢ä¸º CUDA-safe ç‰ˆæœ¬
                            if isinstance(layer.mlp, CudaSafeLinear):
                                print(f"ğŸ”¬ forward_attn_layer: MLP æ˜¯ CudaSafeLinear ç±»å‹")
                                hidden_states = layer.mlp(hidden_states)
                                print(f"ğŸ”¬ forward_attn_layer: MLP å¤„ç†å®Œæˆï¼Œå½¢çŠ¶: {hidden_states.shape}")
                            else:
                                print(f"ğŸ”¬ forward_attn_layer: MLP ä¸æ˜¯ CudaSafeLinear ç±»å‹ï¼Œä½¿ç”¨æ‰‹åŠ¨å®ç°")
                                print(f"ğŸ”¬ forward_attn_layer: å½“å‰ MLP ç±»å‹: {type(layer.mlp)}")
                                print(f"ğŸ”¬ forward_attn_layer: å»ºè®®é‡æ–°è¿è¡Œ _enable_cuda_safe_qkvo_linears()")
                                # æ‰‹åŠ¨å®ç° MLP ä»¥é¿å… cuBLAS
                                from .ops import custom_mlp_forward as _cmf
                                hidden_states = _cmf(hidden_states, layer.mlp)
                                print(f"ğŸ”¬ forward_attn_layer: æ‰‹åŠ¨ MLP å¤„ç†å®Œæˆï¼Œå½¢çŠ¶: {hidden_states.shape}")
                        else:
                            # ä½¿ç”¨æ ‡å‡†çš„ linear1 å’Œ linear2ï¼Œä½†ç¡®ä¿å®ƒä»¬æ˜¯ CUDA-safe çš„
                            print(f"ğŸ”¬ forward_attn_layer: ä½¿ç”¨æ ‡å‡† MLP ç»“æ„ï¼Œç¡®ä¿ CUDA-safe...")
                            if hasattr(layer, 'linear1') and hasattr(layer, 'linear2'):
                                print(f"ğŸ”¬ forward_attn_layer: linear1 ç±»å‹: {type(layer.linear1)}")
                                print(f"ğŸ”¬ forward_attn_layer: linear2 ç±»å‹: {type(layer.linear2)}")
                                # æ£€æŸ¥æ˜¯å¦å·²ç»è¢«æ›¿æ¢ä¸º CUDA-safe ç‰ˆæœ¬
                                if isinstance(layer.linear1, CudaSafeLinear) and isinstance(layer.linear2, CudaSafeLinear):
                                    print(f"ğŸ”¬ forward_attn_layer: æ ‡å‡† MLP æ˜¯ CudaSafeLinear ç±»å‹")
                                    hidden_states = layer.linear1(hidden_states)
                                    hidden_states = layer.activation(hidden_states)
                                    hidden_states = layer.dropout(hidden_states)
                                    hidden_states = layer.linear2(hidden_states)
                                    hidden_states = layer.dropout1(hidden_states)
                                    print(f"ğŸ”¬ forward_attn_layer: æ ‡å‡† MLP å¤„ç†å®Œæˆï¼Œå½¢çŠ¶: {hidden_states.shape}")
                                else:
                                    print(f"ğŸ”¬ forward_attn_layer: æ ‡å‡† MLP ä¸æ˜¯ CudaSafeLinear ç±»å‹ï¼Œä½¿ç”¨æ‰‹åŠ¨å®ç°")
                                    print(f"ğŸ”¬ forward_attn_layer: å»ºè®®é‡æ–°è¿è¡Œ _enable_cuda_safe_qkvo_linears()")
                                    # æ‰‹åŠ¨å®ç°ä»¥é¿å… cuBLAS
                                    from .ops import custom_linear_forward as _clf
                                    hidden_states = _clf(hidden_states, layer.linear1)
                                    hidden_states = layer.activation(hidden_states)
                                    hidden_states = layer.dropout(hidden_states)
                                    hidden_states = _clf(hidden_states, layer.linear2)
                                    hidden_states = layer.dropout1(hidden_states)
                                    print(f"ğŸ”¬ forward_attn_layer: æ‰‹åŠ¨æ ‡å‡† MLP å¤„ç†å®Œæˆï¼Œå½¢çŠ¶: {hidden_states.shape}")
                    except Exception as e:
                        print(f"âŒ forward_attn_layer: MLP å¤„ç†å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        return None, None
                    
                    # ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥ï¼šMLP è¾“å‡º + ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥åçš„çŠ¶æ€
                    try:
                        print(f"ğŸ”¬ forward_attn_layer: åº”ç”¨ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥...")
                        hidden_states = hidden_states + after_first_residual
                        print(f"ğŸ”¬ forward_attn_layer: ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥å®Œæˆï¼Œå½¢çŠ¶: {hidden_states.shape}")
                    except Exception as e:
                        print(f"âŒ forward_attn_layer: ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        return None, None
                    
                    print(f"ğŸ”¬ forward_attn_layer: ç¬¬ {i} ä¸ªembedding å¤„ç†å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {hidden_states.shape}")
                    att_outputs.append(hidden_states)
                else:
                    print(f"âŒ forward_attn_layer: ç¬¬ {i} ä¸ªembedding ç¼ºå°‘å¿…è¦çš„æŠ•å½±å±‚ï¼Œè·³è¿‡å¤„ç†")
                    att_outputs.append(None)
            else:
                print(f"âŒ forward_attn_layer: ç¬¬ {i} ä¸ªembedding ç¼ºå°‘ self_attn å±æ€§ï¼Œè·³è¿‡å¤„ç†")
                att_outputs.append(None)
            
            # å¤„ç† KV ç¼“å­˜
            if use_cache and fill_kv_cache:
                if past_key_values is not None and past_key_values[i] is not None:
                    past_key_values_list.append(past_key_values[i])
                else:
                    past_key_values_list.append(None)
            else:
                past_key_values_list.append(None)
        
        print(f"ğŸ”¬ forward_attn_layer: ç¬¬ {layer_idx} å±‚å®Œæˆ")
        return att_outputs, past_key_values_list

    def forward_cross_attn_layer(
        self,
        model_layers,
        inputs_embeds,
        layer_idx,
        position_ids,
        attention_mask,
        batch_size,
        head_dim,
        use_cache: bool = True,
        fill_kv_cache: bool = True,
        past_key_values=None,
    ) -> List[torch.Tensor]:
        """å‰å‘ä¼ æ’­äº¤å‰æ³¨æ„åŠ›å±‚"""
        print = self._dbg_print
        print(f"ğŸ”¬ forward_cross_attn_layer: ç¬¬ {layer_idx} å±‚å¼€å§‹...")
        
        attention_interface = self.get_attention_interface()

        att_outputs = []
        past_key_values_list = []
        
        assert len(inputs_embeds) == 2 or (use_cache and past_key_values is not None and not fill_kv_cache), (
            f"Both len(inputs_embeds) == {len(inputs_embeds)} and past_key_values is {past_key_values}"
        )

        if len(inputs_embeds) == 2 and not past_key_values:
            # Prefix attention
            seq_len = inputs_embeds[0].shape[1]
            position_id, expert_position_id = position_ids[:, :seq_len], position_ids[:, seq_len:]
            prefix_attention_mask = attention_mask[:, :seq_len, :seq_len]

            layer = model_layers[0][layer_idx]

            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ å…¼å®¹æ€§å¤„ç†ï¼Œæ”¯æŒä¸åŒç±»å‹çš„å±‚
            if hasattr(layer, 'input_layernorm'):
                # æ ‡å‡† TransformerEncoderLayer
                print(f"ğŸ”¬ forward_cross_attn_layer: ä½¿ç”¨æ ‡å‡† input_layernorm")
                hidden_states = layer.input_layernorm(inputs_embeds[0])
            elif hasattr(layer, 'ln_1'):
                # GPT2 é£æ ¼çš„å±‚
                print(f"ğŸ”¬ forward_cross_attn_layer: ä½¿ç”¨ GPT2 é£æ ¼çš„ ln_1")
                hidden_states = layer.ln_1(inputs_embeds[0])
            elif hasattr(layer, 'norm1'):
                # å…¶ä»–å¯èƒ½çš„å‘½å
                print(f"ğŸ”¬ forward_cross_attn_layer: ä½¿ç”¨ norm1")
                hidden_states = layer.norm1(inputs_embeds[0])
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•åŠ¨æ€æ·»åŠ 
                print(f"âš ï¸ ğŸ”¬ forward_cross_attn_layer: å±‚ {type(layer)} æ²¡æœ‰æ ‡å‡†å½’ä¸€åŒ–å±æ€§ï¼Œå°è¯•åŠ¨æ€æ·»åŠ ...")
                if hasattr(layer, 'ln_1'):
                    layer.input_layernorm = layer.ln_1
                    print(f"ğŸ”¬ forward_cross_attn_layer: åŠ¨æ€æ·»åŠ  input_layernorm = ln_1")
                    hidden_states = layer.input_layernorm(inputs_embeds[0])
                elif hasattr(layer, 'norm1'):
                    layer.input_layernorm = layer.norm1
                    print(f"ğŸ”¬ forward_cross_attn_layer: åŠ¨æ€æ·»åŠ  input_layernorm = norm1")
                    hidden_states = layer.input_layernorm(inputs_embeds[0])
                else:
                    print(f"âŒ ğŸ”¬ forward_cross_attn_layer: æ— æ³•æ‰¾åˆ°åˆé€‚çš„å½’ä¸€åŒ–å±‚ï¼Œè·³è¿‡å¤„ç†")
                    # æ— æ³•å¤„ç†ï¼Œè¿”å›ç©ºç»“æœ
                    att_outputs.append(None)
                    past_key_values_list.append(None)
                    return att_outputs, past_key_values_list

            input_shape = hidden_states.shape[:-1]
            hidden_shape = (*input_shape, -1, layer.self_attn.head_dim)

            hidden_states = hidden_states.to(dtype=layer.self_attn.q_proj.weight.dtype)
            query_state = layer.self_attn.q_proj(hidden_states).view(hidden_shape)
            key_state = layer.self_attn.k_proj(hidden_states).view(hidden_shape)
            value_state = layer.self_attn.v_proj(hidden_states).view(hidden_shape)

            # B,L,H,D with L sequence length, H number of heads, D head dim
            query_states = apply_rope(query_state, position_id)
            key_states = apply_rope(key_state, position_id)

            att_output = attention_interface(
                prefix_attention_mask, batch_size, head_dim, query_states, key_states, value_state
            )
            
            # åº”ç”¨è¾“å‡ºæŠ•å½±å±‚å’Œæ®‹å·®è¿æ¥
            att_output = layer.self_attn.o_proj(att_output)
            hidden_states = att_output + inputs_embeds[0]
            
            # ä¿å­˜ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥åçš„çŠ¶æ€
            after_first_residual = hidden_states.clone()
            
            # åº”ç”¨åæ³¨æ„åŠ›å±‚å½’ä¸€åŒ–å’Œ MLP
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ å…¼å®¹æ€§å¤„ç†ï¼Œæ”¯æŒä¸åŒç±»å‹çš„å±‚
            if hasattr(layer, 'post_attention_layernorm'):
                # æ ‡å‡† TransformerEncoderLayer
                print(f"ğŸ”¬ forward_cross_attn_layer: ä½¿ç”¨æ ‡å‡† post_attention_layernorm")
                hidden_states = layer.post_attention_layernorm(hidden_states)
            elif hasattr(layer, 'ln_2'):
                # GPT2 é£æ ¼çš„å±‚
                print(f"ğŸ”¬ forward_cross_attn_layer: ä½¿ç”¨ GPT2 é£æ ¼çš„ ln_2")
                hidden_states = layer.ln_2(hidden_states)
            elif hasattr(layer, 'norm2'):
                # å…¶ä»–å¯èƒ½çš„å‘½å
                print(f"ğŸ”¬ forward_cross_attn_layer: ä½¿ç”¨ norm2")
                hidden_states = layer.norm2(hidden_states)
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•åŠ¨æ€æ·»åŠ 
                print(f"âš ï¸ ğŸ”¬ forward_cross_attn_layer: å±‚ {type(layer)} æ²¡æœ‰æ ‡å‡† post_attention_layernorm å±æ€§ï¼Œå°è¯•åŠ¨æ€æ·»åŠ ...")
                if hasattr(layer, 'ln_2'):
                    layer.post_attention_layernorm = layer.ln_2
                    print(f"ğŸ”¬ forward_cross_attn_layer: åŠ¨æ€æ·»åŠ  post_attention_layernorm = ln_2")
                    hidden_states = layer.post_attention_layernorm(hidden_states)
                elif hasattr(layer, 'norm2'):
                    layer.post_attention_layernorm = layer.norm2
                    print(f"ğŸ”¬ forward_cross_attn_layer: åŠ¨æ€æ·»åŠ  post_attention_layernorm = norm2")
                    hidden_states = layer.post_attention_layernorm(hidden_states)
                else:
                    print(f"âŒ ğŸ”¬ forward_cross_attn_layer: æ— æ³•æ‰¾åˆ°åˆé€‚çš„åæ³¨æ„åŠ›å½’ä¸€åŒ–å±‚ï¼Œè·³è¿‡å¤„ç†")
                    att_outputs.append(None)
                    return att_outputs, past_key_values_list
            
            if hasattr(layer, 'mlp'):
                hidden_states = layer.mlp(hidden_states)
            else:
                hidden_states = layer.linear1(hidden_states)
                hidden_states = layer.activation(hidden_states)
                hidden_states = layer.dropout(hidden_states)
                hidden_states = layer.linear2(hidden_states)
                hidden_states = layer.dropout1(hidden_states)
            
            # ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥
            hidden_states = hidden_states + after_first_residual
            
            att_outputs.append(hidden_states)
            past_key_values_list.append(None)
        else:
            expert_position_id = position_ids
            att_outputs.append(None)
            past_key_values_list.append(None)

        if use_cache and past_key_values is None:
            past_key_values = {}

        if use_cache:
            if fill_kv_cache:
                past_key_values[layer_idx] = {
                    "key_states": key_states,
                    "value_states": value_states,
                }
            else:
                # TODO here, some optimization can be done - similar to a `StaticCache` we can declare the `max_len` before.
                # so we create an empty cache, with just one cuda malloc, and if (in autoregressive case) we reach
                # the max len, then we (for instance) double the cache size. This implementation already exists
                # in `transformers`. (molbap)
                key_states = past_key_values[layer_idx]["key_states"]
                value_states = past_key_values[layer_idx]["value_states"]

        # Expert
        expert_layer = model_layers[1][layer_idx]
        # ä»…å½“æä¾›äº† expert çš„è¾“å…¥æ—¶æ‰æ‰§è¡Œä¸“å®¶åˆ†æ”¯ï¼Œé¿å…é¢„å¡«ç¼“å­˜é˜¶æ®µè®¿é—® None
        if expert_layer is not None and inputs_embeds[1] is not None:
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ å…¼å®¹æ€§å¤„ç†ï¼Œæ”¯æŒä¸åŒç±»å‹çš„å±‚
            if hasattr(expert_layer, 'input_layernorm'):
                # æ ‡å‡† TransformerEncoderLayer
                print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚ä½¿ç”¨æ ‡å‡† input_layernorm")
                expert_hidden_states = expert_layer.input_layernorm(inputs_embeds[1])
            elif hasattr(expert_layer, 'ln_1'):
                # GPT2 é£æ ¼çš„å±‚
                print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚ä½¿ç”¨ GPT2 é£æ ¼çš„ ln_1")
                expert_hidden_states = expert_layer.ln_1(inputs_embeds[1])
            elif hasattr(expert_layer, 'norm1'):
                # å…¶ä»–å¯èƒ½çš„å‘½å
                print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚ä½¿ç”¨ norm1")
                expert_hidden_states = expert_layer.norm1(inputs_embeds[1])
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•åŠ¨æ€æ·»åŠ 
                print(f"âš ï¸ ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚ {type(expert_layer)} æ²¡æœ‰æ ‡å‡†å½’ä¸€åŒ–å±æ€§ï¼Œå°è¯•åŠ¨æ€æ·»åŠ ...")
                if hasattr(expert_layer, 'ln_1'):
                    expert_layer.input_layernorm = expert_layer.ln_1
                    print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚åŠ¨æ€æ·»åŠ  input_layernorm = ln_1")
                    expert_hidden_states = expert_layer.input_layernorm(inputs_embeds[1])
                elif hasattr(expert_layer, 'norm1'):
                    expert_layer.input_layernorm = expert_layer.norm1
                    print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚åŠ¨æ€æ·»åŠ  input_layernorm = norm1")
                    expert_hidden_states = expert_layer.input_layernorm(inputs_embeds[1])
                else:
                    print(f"âŒ ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚æ— æ³•æ‰¾åˆ°åˆé€‚çš„å½’ä¸€åŒ–å±‚ï¼Œè·³è¿‡å¤„ç†")
                    att_outputs.append(None)
                    return att_outputs, past_key_values_list

            expert_input_shape = expert_hidden_states.shape[:-1]
            expert_hidden_shape = (*expert_input_shape, -1, expert_layer.self_attn.head_dim)

            expert_hidden_states = expert_hidden_states.to(dtype=expert_layer.self_attn.q_proj.weight.dtype)
            expert_query_state = expert_layer.self_attn.q_proj(expert_hidden_states).view(expert_hidden_shape)

            _key_states = key_states.to(dtype=expert_layer.self_attn.k_proj.weight.dtype).view(
                *key_states.shape[:2], -1
            )
            expert_key_states = expert_layer.self_attn.k_proj(_key_states).view(
                *_key_states.shape[:-1], -1, expert_layer.self_attn.head_dim
            )  # k_proj should have same dim as kv

            _value_states = value_states.to(dtype=expert_layer.self_attn.v_proj.weight.dtype).view(
                *value_states.shape[:2], -1
            )
            expert_value_states = expert_layer.self_attn.v_proj(_value_states).view(
                *_value_states.shape[:-1], -1, expert_layer.self_attn.head_dim
            )

            expert_position_id = (
                expert_position_id - torch.min(expert_position_id, dim=1, keepdim=True).values
            )  # start from 0
            expert_attention_mask = attention_mask[
                :, -inputs_embeds[1].shape[1] :, : expert_key_states.shape[1] :
            ]  # take into account kv

            expert_query_states = apply_rope(expert_query_state, expert_position_id)

            expert_att_output = attention_interface(
                expert_attention_mask, batch_size, head_dim, expert_query_states, expert_key_states, expert_value_states
            )
            
            # åº”ç”¨è¾“å‡ºæŠ•å½±å±‚å’Œæ®‹å·®è¿æ¥
            expert_att_output = expert_layer.self_attn.o_proj(expert_att_output)
            expert_hidden_states = expert_att_output + inputs_embeds[1]
            
            # ä¿å­˜ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥åçš„çŠ¶æ€
            after_first_residual = expert_hidden_states.clone()
            
            # åº”ç”¨åæ³¨æ„åŠ›å±‚å½’ä¸€åŒ–å’Œ MLP
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ å…¼å®¹æ€§å¤„ç†ï¼Œæ”¯æŒä¸åŒç±»å‹çš„å±‚
            if hasattr(expert_layer, 'post_attention_layernorm'):
                # æ ‡å‡† TransformerEncoderLayer
                print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚ä½¿ç”¨æ ‡å‡† post_attention_layernorm")
                expert_hidden_states = expert_layer.post_attention_layernorm(expert_hidden_states)
            elif hasattr(expert_layer, 'ln_2'):
                # GPT2 é£æ ¼çš„å±‚
                print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚ä½¿ç”¨ GPT2 é£æ ¼çš„ ln_2")
                expert_hidden_states = expert_layer.ln_2(expert_hidden_states)
            elif hasattr(expert_layer, 'norm2'):
                # å…¶ä»–å¯èƒ½çš„å‘½å
                print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚ä½¿ç”¨ norm2")
                expert_hidden_states = expert_layer.norm2(expert_hidden_states)
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•åŠ¨æ€æ·»åŠ 
                print(f"âš ï¸ ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚ {type(expert_layer)} æ²¡æœ‰æ ‡å‡† post_attention_layernorm å±æ€§ï¼Œå°è¯•åŠ¨æ€æ·»åŠ ...")
                if hasattr(expert_layer, 'ln_2'):
                    expert_layer.post_attention_layernorm = expert_layer.ln_2
                    print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚åŠ¨æ€æ·»åŠ  post_attention_layernorm = ln_2")
                    expert_hidden_states = expert_layer.post_attention_layernorm(expert_hidden_states)
                elif hasattr(expert_layer, 'norm2'):
                    expert_layer.post_attention_layernorm = expert_layer.norm2
                    print(f"ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚åŠ¨æ€æ·»åŠ  post_attention_layernorm = norm2")
                    expert_hidden_states = expert_layer.post_attention_layernorm(expert_hidden_states)
                else:
                    print(f"âŒ ğŸ”¬ forward_cross_attn_layer: ä¸“å®¶å±‚æ— æ³•æ‰¾åˆ°åˆé€‚çš„åæ³¨æ„åŠ›å½’ä¸€åŒ–å±‚ï¼Œè·³è¿‡å¤„ç†")
                    att_outputs.append(None)
                    return att_outputs, past_key_values_list
            
            if hasattr(expert_layer, 'mlp'):
                expert_hidden_states = expert_layer.mlp(expert_hidden_states)
            else:
                expert_hidden_states = expert_layer.linear1(expert_hidden_states)
                expert_hidden_states = expert_layer.activation(expert_hidden_states)
                expert_hidden_states = expert_layer.dropout(expert_hidden_states)
                expert_hidden_states = expert_layer.linear2(expert_hidden_states)
                expert_hidden_states = expert_layer.dropout1(expert_hidden_states)
            
            # ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥
            expert_hidden_states = expert_hidden_states + after_first_residual
            
            att_outputs.append(expert_hidden_states)
        else:
            att_outputs.append(None)
        
        # å¤„ç† KV ç¼“å­˜
        if use_cache and fill_kv_cache:
            if past_key_values is not None and len(past_key_values_list) < len(att_outputs):
                past_key_values_list.append(past_key_values.get(layer_idx, None))
            else:
                past_key_values_list.append(None)
        else:
            past_key_values_list.append(None)
        
        print(f"ğŸ”¬ forward_cross_attn_layer: ç¬¬ {layer_idx} å±‚å®Œæˆ")
        # è¿”å› KV å­—å…¸ç”¨äºåç»­ cross-attn å¤ç”¨
        return att_outputs, past_key_values

    def get_model_layers(self, models: list) -> list:
        vlm_layers = []
        expert_layers = []
        multiple_of = self.num_vlm_layers // self.num_expert_layers
        for i in range(self.num_vlm_layers):
            if multiple_of > 0 and i > 0 and i % multiple_of != 0:
                expert_layer = None
            else:
                expert_layer_index = i // multiple_of if multiple_of > 0 else i
                # Use correct layer attribute for both models
            if hasattr(models[1], 'layers'):
                expert_layer = models[1].layers[expert_layer_index]
            elif hasattr(models[1], 'h'):
                expert_layer = models[1].h[expert_layer_index]
            else:
                expert_layer = None
                
            if hasattr(models[0], 'layers'):
                vlm_layers.append(models[0].layers[i])
            elif hasattr(models[0], 'h'):
                vlm_layers.append(models[0].h[i])
            else:
                vlm_layers.append(None)
            expert_layers.append(expert_layer)
        return [vlm_layers, expert_layers]

    def forward(
        self,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[List[torch.FloatTensor]] = None,
        use_cache: Optional[bool] = None,
        fill_kv_cache: Optional[bool] = None,
    ):
        # ğŸš€ åºåˆ—é•¿åº¦ä¼˜åŒ–ï¼šæ£€æŸ¥å¹¶é™åˆ¶åºåˆ—é•¿åº¦ä»¥é¿å… Jetson å¹³å°æ€§èƒ½ç“¶é¢ˆ
        max_seq_length = self.max_seq_length
        if inputs_embeds and inputs_embeds[0] is not None:
            seq_len = inputs_embeds[0].shape[1]
            if seq_len > max_seq_length:
                self._seq_len_warn_printed = True
                # æˆªæ–­åºåˆ—é•¿åº¦
                inputs_embeds[0] = inputs_embeds[0][:, :max_seq_length, :]
                # åŒæ—¶è°ƒæ•´ attention_mask å’Œ position_ids
                if attention_mask is not None:
                    attention_mask = attention_mask[:, :max_seq_length, :max_seq_length]
                if position_ids is not None:
                    position_ids = position_ids[:, :max_seq_length]
                self._seq_len_truncate_printed = True
        
        models = [self.get_vlm_model().text_model, self.lm_expert]
        model_layers = self.get_model_layers(models)
        for hidden_states in inputs_embeds:
            # TODO this is very inefficient
            # dtype is always the same, batch size too (if > 1 len)
            # device could be trickier in multi gpu edge cases but that's it
            if hidden_states is None:
                continue
            batch_size = hidden_states.shape[0]

        # RMSNorm
        num_layers = self.num_vlm_layers
        head_dim = self.vlm.config.text_config.head_dim
        # ç»‘å®šå—å®ç°ï¼ˆä¸€æ¬¡ç»‘å®šï¼Œå¾ªç¯å†…å¤ç”¨ï¼‰
        if not hasattr(self, "_cross_attn_blocks"):
            self._cross_attn_blocks = CrossAttnBlocks(
                self._dbg_print,
                self.get_model_layers,
                self.num_vlm_layers,
                self.num_attention_heads,
                self.num_key_value_heads,
                custom_mlp_forward=custom_mlp_forward,
                custom_linear_forward=custom_linear_forward,
            )

        for layer_idx in range(num_layers):
            try:
                import os as _os
                if int(_os.getenv("EVAL_VERBOSE", "0")) >= 1:
                    print(f"ğŸ”¬ VLM Forward: å¼€å§‹å¤„ç†ç¬¬ {layer_idx} å±‚...")
            except Exception:
                pass
            if (
                "cross" not in self.attention_mode
                or (self.self_attn_every_n_layers > 0 and layer_idx % self.self_attn_every_n_layers == 0)
            ):
                # forward_attn_layer å·²ç»å®Œæˆäº†å®Œæ•´çš„å±‚å¤„ç†ï¼ˆåŒ…æ‹¬ MLPï¼‰ï¼Œç›´æ¥ä½¿ç”¨å…¶è¾“å‡º
                # æ³¨æ„ï¼šä¸è¦åœ¨æ­¤è¦†ç›–å…¨å±€ KV å­—å…¸ç¼“å­˜
                outputs_embeds, _ = self._cross_attn_blocks.forward_attn_layer(
                    model_layers,
                    inputs_embeds,
                    layer_idx,
                    position_ids,
                    attention_mask,
                    batch_size,
                    head_dim,
                    use_cache=use_cache,
                    fill_kv_cache=fill_kv_cache,
                    past_key_values=past_key_values,
                )
            else:
                outputs_embeds, past_key_values = self._cross_attn_blocks.forward_cross_attn_layer(
                    model_layers,
                    inputs_embeds,
                    layer_idx,
                    position_ids,
                    attention_mask,
                    batch_size,
                    head_dim,
                    use_cache=use_cache,
                    fill_kv_cache=fill_kv_cache,
                    past_key_values=past_key_values,
                )
            
            # ç›´æ¥ä½¿ç”¨ forward_attn_layer çš„è¾“å‡ºï¼Œå› ä¸ºå®ƒå·²ç»å®Œæˆäº†å®Œæ•´çš„å±‚å¤„ç†
            # åŒ…æ‹¬ï¼šinput_layernorm -> attention -> post_attention_layernorm -> MLP
            # ä¸éœ€è¦åœ¨è¿™é‡Œé‡å¤å¤„ç†
            inputs_embeds = outputs_embeds

        # final norm
        outputs_embeds = []
        for i, hidden_states in enumerate(inputs_embeds):
            if hidden_states is not None:
                out_emb = models[i].norm(hidden_states)
                outputs_embeds.append(out_emb)
            else:
                outputs_embeds.append(None)
        return outputs_embeds, past_key_values

    def get_attention_interface(self):
        # Attention é€»è¾‘ç»Ÿä¸€ç”± AttentionBackend è´Ÿè´£
        backend = AttentionBackend(self._dbg_print, self.num_attention_heads, self.num_key_value_heads)
        return backend.get()

    def custom_efficient_attention_forward(
        self, attention_mask, batch_size, head_dim, query_states, key_states, value_states
    ):
        print = self._dbg_print
        """è‡ªå®šä¹‰é«˜æ•ˆattentionå®ç°ï¼Œç»•è¿‡CUBLASä¾èµ–"""
        print(f"ğŸš€ ä½¿ç”¨è‡ªå®šä¹‰é«˜æ•ˆattentionè®¡ç®—...")
        # å¼ºåˆ¶ä»…èµ° GPU è·¯å¾„ï¼Œç¦æ­¢ä»»ä½• CPU å°è¯•
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA ä¸å¯ç”¨ï¼Œç¦æ­¢åœ¨ CPU ä¸Šè¿è¡Œ attention")
        if not (query_states.is_cuda and key_states.is_cuda and value_states.is_cuda):
            raise RuntimeError(
                f"Attention å¼ é‡å¿…é¡»åœ¨ CUDA ä¸Š: Q:{query_states.device}, K:{key_states.device}, V:{value_states.device}"
            )
        
        # å®Œå…¨ç¦ç”¨ cuBLAS ç›¸å…³ä¼˜åŒ–
        try:
            torch.cuda.synchronize()
        except Exception:
            pass
        # å…³é—­æ‰€æœ‰å¯èƒ½å¯¼è‡´ cuBLAS é—®é¢˜çš„ä¼˜åŒ–
        try:
            torch.backends.cuda.matmul.allow_tf32 = False
            torch.backends.cudnn.allow_tf32 = False
            torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False
            torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False
        except Exception:
            pass
        
        # æ‰“å°å…³é”®å½¢çŠ¶/ç±»å‹ï¼Œä¾¿äºç²¾ç¡®å®šä½
        print(
            f"[attn] devices Q:{query_states.device} K:{key_states.device} V:{value_states.device}; dtypes Q:{query_states.dtype} K:{key_states.dtype} V:{value_states.dtype}"
        )
        print(
            f"[attn] shapes Q:{query_states.shape} K:{key_states.shape} V:{value_states.shape}"
        )
        
        # å¤„ç†è¾“å…¥å¼ é‡ç»´åº¦ï¼Œç¡®ä¿æ˜¯ 4D æ ¼å¼ [batch, seq_len, num_heads, head_dim]
        if query_states.dim() == 3:
            # 3D è¾“å…¥ï¼š[batch, seq_len, hidden_size] -> [batch, seq_len, num_heads, head_dim]
            hidden_size = query_states.shape[-1]
            num_heads = hidden_size // head_dim
            if hidden_size % head_dim != 0:
                raise ValueError(f"hidden_size ({hidden_size}) å¿…é¡»èƒ½è¢« head_dim ({head_dim}) æ•´é™¤")
            
            query_states = query_states.view(batch_size, -1, num_heads, head_dim)
            key_states = key_states.view(batch_size, -1, num_heads, head_dim)
            value_states = value_states.view(batch_size, -1, num_heads, head_dim)
        
        num_att_heads = query_states.shape[2]
        num_key_value_heads = key_states.shape[2]
        num_key_value_groups = num_att_heads // num_key_value_heads

        sequence_length = key_states.shape[1]

        # é‡å¡‘keyå’Œvalue states
        key_states = key_states[:, :, :, None, :].expand(
            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim
        )
        key_states = key_states.reshape(
            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim
        )

        value_states = value_states[:, :, :, None, :].expand(
            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim
        )
        value_states = value_states.reshape(
            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim
        )

        # è½¬æ¢ç»´åº¦: (batch, seq_len, heads, head_dim) -> (batch, heads, seq_len, head_dim)
        query_states = query_states.transpose(1, 2)
        key_states = key_states.transpose(1, 2)
        value_states = value_states.transpose(1, 2)

        # é«˜æ•ˆçš„attentionè®¡ç®—ï¼Œå®Œå…¨ç»•è¿‡ cuBLASï¼šä¸ä½¿ç”¨ matmul/bmm/einsum
        # ä½¿ç”¨æ›´ä½ç²¾åº¦è®¡ç®—ï¼Œä½†ä¿æŒæ•°å€¼ç¨³å®šæ€§
        scale = head_dim ** -0.5

        # æŸ¥è¯¢ä¸é”®çš„åŒé‡åˆ†å—ï¼Œé™ä½æ˜¾å­˜ä¸é¿å…å¤§å¼ é‡ GEMM
        # æ ¹æ® Jetson æ˜¾å­˜æƒ…å†µè°ƒæ•´åˆ†å—å¤§å°
        q_chunk_size = 16  # å‡å°æŸ¥è¯¢åˆ†å—ï¼Œé™ä½æ˜¾å­˜å³°å€¼
        k_block_size = 64  # å‡å°é”®åˆ†å—ï¼Œé¿å…å¤§çŸ©é˜µè¿ç®—
        bsize, num_heads_total, q_len, d = query_states.shape
        k_len = key_states.shape[2]

        att_output = torch.empty(
            (bsize, num_heads_total, q_len, d), device=query_states.device, dtype=value_states.dtype
        )

        neg_inf = torch.finfo(torch.float32).min

        for qi in range(0, q_len, q_chunk_size):
            qj = min(qi + q_chunk_size, q_len)
            q_chunk = query_states[:, :, qi:qj, :].to(torch.float32)  # [B,H,Qc,D]
            # ç¬¬ä¸€éï¼šè®¡ç®—åˆ†å—æœ€å¤§å€¼ mï¼Œç”¨äºç¨³å®š softmaxï¼ˆè·¨æ‰€æœ‰ Kï¼‰
            m = torch.full((bsize, num_heads_total, q_chunk.shape[2]), neg_inf, device=q_chunk.device)

            for kk in range(0, k_len, k_block_size):
                kl = min(kk + k_block_size, k_len)
                k_block = key_states[:, :, kk:kl, :].to(torch.float32)  # [B,H,Kb,D]
                # ç‚¹ç§¯ï¼šé€šè¿‡é€å…ƒç´ ä¹˜ä¸å½’çº¦å®ç°ï¼Œé¿å… GEMM
                # -> logits_block: [B,H,Qc,Kb]
                logits_block = (q_chunk.unsqueeze(3) * k_block.unsqueeze(2)).sum(dim=-1) * scale
                if attention_mask is not None:
                    mask_sub = attention_mask[:, qi:qj, kk:kl]  # [B,Qc,Kb]
                    logits_block = torch.where(
                        mask_sub.unsqueeze(1), logits_block, torch.tensor(neg_inf, device=logits_block.device)
                    )
                # è·¨ Kb çš„æœ€å¤§å€¼
                block_max = logits_block.max(dim=3).values  # [B,H,Qc]
                m = torch.maximum(m, block_max)
                del k_block, logits_block, block_max
                torch.cuda.empty_cache()

            # ç¬¬äºŒéï¼šç´¯è®¡åˆ†æ¯ S ä¸åˆ†å­ Nï¼Œå®ç°ç¨³å®š softmaxï¼Œå†ä¸ V åšåŠ æƒæ±‚å’Œï¼ŒåŒæ ·é¿å… GEMM
            S = torch.zeros((bsize, num_heads_total, q_chunk.shape[2]), device=q_chunk.device, dtype=torch.float32)
            N = torch.zeros((bsize, num_heads_total, q_chunk.shape[2], d), device=q_chunk.device, dtype=torch.float32)

            for kk in range(0, k_len, k_block_size):
                kl = min(kk + k_block_size, k_len)
                k_block = key_states[:, :, kk:kl, :].to(torch.float32)  # [B,H,Kb,D]
                v_block = value_states[:, :, kk:kl, :].to(torch.float32)  # [B,H,Kb,D]

                logits_block = (q_chunk.unsqueeze(3) * k_block.unsqueeze(2)).sum(dim=-1) * scale  # [B,H,Qc,Kb]
                if attention_mask is not None:
                    mask_sub = attention_mask[:, qi:qj, kk:kl]  # [B,Qc,Kb]
                    logits_block = torch.where(
                        mask_sub.unsqueeze(1), logits_block, torch.tensor(neg_inf, device=logits_block.device)
                    )

                # ç¨³å®šçš„ exp(logits - m)
                exp_block = torch.exp(logits_block - m.unsqueeze(3))  # [B,H,Qc,Kb]
                S = S + exp_block.sum(dim=3)  # [B,H,Qc]

                # è®¡ç®—åŠ æƒå€¼ï¼šé¿å… GEMMï¼Œä½¿ç”¨é€å…ƒç´ ä¹˜å¹¶æ²¿ Kb å½’çº¦
                # exp_block: [B,H,Qc,Kb] -> [B,H,Qc,Kb,1]
                # v_block:   [B,H,Kb,D]  -> [B,H,1,Kb,D]
                weighted_v = (exp_block.unsqueeze(-1) * v_block.unsqueeze(2)).sum(dim=3)  # [B,H,Qc,D]
                N = N + weighted_v

                del k_block, v_block, logits_block, exp_block, weighted_v
                torch.cuda.empty_cache()

            # å½’ä¸€åŒ–ï¼Œå¾—åˆ°è¯¥ Q chunk çš„è¾“å‡º
            att_out_chunk = (N / (S.unsqueeze(-1) + 1e-8)).to(value_states.dtype)  # [B,H,Qc,D]
            att_output[:, :, qi:qj, :] = att_out_chunk
            del q_chunk, m, S, N, att_out_chunk
            torch.cuda.empty_cache()

        # ç°åœ¨ att_output: [B,H,Lq,D]
        
        # è½¬æ¢å›åŸå§‹ç»´åº¦: (batch, heads, seq_len, head_dim) -> (batch, seq_len, heads, head_dim)
        att_output = att_output.transpose(1, 2)
        
        # é‡å¡‘ä¸ºæœ€ç»ˆå½¢çŠ¶
        att_output = att_output.reshape(batch_size, -1, num_att_heads * head_dim)
        
        print(f"âœ… è‡ªå®šä¹‰attentionè®¡ç®—æˆåŠŸ")
        return att_output

    def optimized_attention_forward(
        self, attention_mask, batch_size, head_dim, query_states, key_states, value_states
    ):
        print = self._dbg_print
        """é«˜æ•ˆçš„attentionå®ç°ï¼Œä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å‡½æ•°"""
        print(f"ğŸš€ ä½¿ç”¨ä¼˜åŒ–çš„attentionè®¡ç®—...")
        
        num_att_heads = self.num_attention_heads
        num_key_value_heads = self.num_key_value_heads
        num_key_value_groups = num_att_heads // num_key_value_heads

        sequence_length = key_states.shape[1]

        # é‡å¡‘keyå’Œvalue states
        key_states = key_states[:, :, :, None, :].expand(
            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim
        )
        key_states = key_states.reshape(
            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim
        )

        value_states = value_states[:, :, :, None, :].expand(
            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim
        )
        value_states = value_states.reshape(
            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim
        )

        # è°ƒæ•´ç»´åº¦é¡ºåº: (batch, seq_len, heads, head_dim) -> (batch, heads, seq_len, head_dim)
        query_states = query_states.transpose(1, 2)  # [batch, heads, seq_len, head_dim]
        key_states = key_states.transpose(1, 2)      # [batch, heads, seq_len, head_dim]
        value_states = value_states.transpose(1, 2)  # [batch, heads, seq_len, head_dim]

        # å¤„ç†attention mask: å°†Falseè½¬æ¢ä¸º-infï¼ŒTrueä¿æŒä¸º0
        if attention_mask is not None:
            # attention_mask shape: [batch, seq_len, seq_len] -> [batch, 1, seq_len, seq_len]
            attn_mask = attention_mask.unsqueeze(1)  # æ·»åŠ headç»´åº¦
            # PyTorchçš„scaled_dot_product_attentionæœŸæœ›çš„maskï¼šTrueè¡¨ç¤ºä¿ç•™ï¼ŒFalseè¡¨ç¤ºmaskæ‰
            # ä½†æˆ‘ä»¬çš„maskä¼¼ä¹æ˜¯ç›¸åçš„ï¼Œéœ€è¦ç¡®è®¤
            # ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬ä¸ä½¿ç”¨maskï¼Œè€Œæ˜¯ç”¨ä¼ ç»Ÿæ–¹å¼
            attn_mask = None  # æš‚æ—¶ç¦ç”¨maskä¼˜åŒ–ï¼Œç¡®ä¿æ­£ç¡®æ€§

        # ä½¿ç”¨PyTorchçš„é«˜æ•ˆattentionå®ç°ï¼ŒåŠ å…¥å†…å­˜ç®¡ç†
        try:
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
            # å°†æ•°æ®è½¬æ¢ä¸ºæ›´èŠ‚çœå†…å­˜çš„dtype
            original_dtype = query_states.dtype
            if original_dtype == torch.float32:
                query_states = query_states.to(torch.bfloat16)
                key_states = key_states.to(torch.bfloat16)
                value_states = value_states.to(torch.bfloat16)
            
            att_output = torch.nn.functional.scaled_dot_product_attention(
                query_states, key_states, value_states,
                attn_mask=attn_mask,
                dropout_p=0.0,  # æ¨ç†æ—¶ä¸ä½¿ç”¨dropout
                is_causal=False  # ä¸æ˜¯å› æœæ³¨æ„åŠ›
            )
            
            # è½¬æ¢å›åŸå§‹dtype
            if original_dtype == torch.float32:
                att_output = att_output.to(original_dtype)
                
            print(f"âœ… scaled_dot_product_attention æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ scaled_dot_product_attention å¤±è´¥ï¼Œå›é€€åˆ°eagerå®ç°: {e}")
            # æ¸…ç†å¯èƒ½çš„ä¸­é—´å˜é‡
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return self.eager_attention_forward(attention_mask, batch_size, head_dim, 
                                              query_states.transpose(1, 2), 
                                              key_states.transpose(1, 2), 
                                              value_states.transpose(1, 2))

        # è°ƒæ•´è¾“å‡ºç»´åº¦: (batch, heads, seq_len, head_dim) -> (batch, seq_len, total_hidden_size)
        att_output = att_output.transpose(1, 2)
        
        # é‡å¡‘ä¸ºæœ€ç»ˆå½¢çŠ¶: (batch, seq_len, total_hidden_size)
        att_output = att_output.reshape(batch_size, -1, num_key_value_heads * num_key_value_groups * head_dim)

        return att_output

    def eager_attention_forward(
        self, attention_mask, batch_size, head_dim, query_states, key_states, value_states
    ):
        print = self._dbg_print
        num_att_heads = self.num_attention_heads
        num_key_value_heads = self.num_key_value_heads
        num_key_value_groups = num_att_heads // num_key_value_heads

        sequence_length = key_states.shape[1]

        key_states = key_states[:, :, :, None, :].expand(
            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim
        )
        key_states = key_states.reshape(
            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim
        )

        value_states = value_states[:, :, :, None, :].expand(
            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim
        )
        value_states = value_states.reshape(
            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim
        )

        # Attention here is upcasted to float32 to match the original eager implementation.
        query_states = query_states.to(dtype=torch.float32)
        key_states = key_states.to(dtype=torch.float32)

        query_states = query_states.transpose(1, 2)
        key_states = key_states.transpose(1, 2)

        att_weights = torch.matmul(query_states, key_states.transpose(2, 3))
        att_weights *= head_dim**-0.5

        att_weights = att_weights.to(dtype=torch.float32)
        big_neg = torch.finfo(att_weights.dtype).min  # -2.3819763e38  # See gemma/modules.py
        masked_att_weights = torch.where(attention_mask[:, None, :, :], att_weights, big_neg)
        probs = nn.functional.softmax(masked_att_weights, dim=-1)
        probs = probs.to(dtype=value_states.dtype)

        att_output = torch.matmul(probs, value_states.permute(0, 2, 1, 3))

        att_output = att_output.permute(0, 2, 1, 3)
        # we use -1 because sequence length can change
        att_output = att_output.reshape(batch_size, -1, num_key_value_heads * num_key_value_groups * head_dim)

        return att_output

    # å·²ç§»é™¤è‡ªå®šä¹‰ç®—å­å®ç°ï¼Œä½¿ç”¨ ops.py æä¾›çš„å‡½æ•°

    def get_performance_optimization_info(self):
        """è·å–æ€§èƒ½ä¼˜åŒ–ä¿¡æ¯å’Œå»ºè®®"""
        info = {
            "max_seq_length": self.max_seq_length,
            "performance_improvement": (305**2) / (self.max_seq_length**2),
            "recommendations": []
        }
        
        if self.max_seq_length <= 64:
            info["recommendations"].append("âœ… åºåˆ—é•¿åº¦è®¾ç½®åˆç†ï¼Œé€‚åˆ Jetson å¹³å°")
        elif self.max_seq_length <= 128:
            info["recommendations"].append("âš ï¸ åºåˆ—é•¿åº¦é€‚ä¸­ï¼Œå¦‚æœä»æœ‰æ€§èƒ½é—®é¢˜å¯è€ƒè™‘å‡å°‘åˆ° 64")
        else:
            info["recommendations"].append("âŒ åºåˆ—é•¿åº¦è¿‡é•¿ï¼Œå»ºè®®å‡å°‘åˆ° 128 æˆ–æ›´å°‘")
        
        if self.max_seq_length > 192:
            info["recommendations"].append("ğŸš¨ å½“å‰è®¾ç½®å¯èƒ½å¯¼è‡´ Jetson å¹³å°æ€§èƒ½ç“¶é¢ˆ")
        
        return info
    
    def print_performance_summary(self):
        """æ‰“å°æ€§èƒ½ä¼˜åŒ–æ€»ç»“"""
        info = self.get_performance_optimization_info()
        print(f"\nğŸš€ [æ€§èƒ½ä¼˜åŒ–æ€»ç»“]")
        print(f"ğŸ“Š æœ€å¤§åºåˆ—é•¿åº¦: {info['max_seq_length']}")
        print(f"ğŸ“ˆ æ€§èƒ½æå‡å€æ•°: {info['performance_improvement']:.1f}x")
        print(f"ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for rec in info["recommendations"]:
            print(f"   {rec}")
        print()
